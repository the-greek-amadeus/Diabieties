{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Diebieties.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tN-lm2LtXPq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "a16f6e4d-ea71-4f15-d96f-bcf9101f808d"
      },
      "source": [
        "#Load libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUZhIJECtadk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "69aee491-5446-4680-ea90-19be43e49391"
      },
      "source": [
        "url = 'https://raw.githubusercontent.com/randerson112358/Python/master/Diabetes/diabetes.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.head(7)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>116</td>\n",
              "      <td>74</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25.6</td>\n",
              "      <td>0.201</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>3</td>\n",
              "      <td>78</td>\n",
              "      <td>50</td>\n",
              "      <td>32</td>\n",
              "      <td>88</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.248</td>\n",
              "      <td>26</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigreeFunction  Age  Outcome\n",
              "0            6      148             72  ...                     0.627   50        1\n",
              "1            1       85             66  ...                     0.351   31        0\n",
              "2            8      183             64  ...                     0.672   32        1\n",
              "3            1       89             66  ...                     0.167   21        0\n",
              "4            0      137             40  ...                     2.288   33        1\n",
              "5            5      116             74  ...                     0.201   30        0\n",
              "6            3       78             50  ...                     0.248   26        1\n",
              "\n",
              "[7 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQqP2Fgxti2p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "633eddd8-e565-44dd-8d7e-9bc6071406cb"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lal4eXAUtsCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop_duplicates(inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MHww3SptuOZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "feb18290-5a1c-4bf0-ba2c-e8c0229d5c16"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(768, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lko3t6mUtvFF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "799b0c06-e90c-4d3b-967a-b687c0860a3d"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pregnancies                 0\n",
              "Glucose                     0\n",
              "BloodPressure               0\n",
              "SkinThickness               0\n",
              "Insulin                     0\n",
              "BMI                         0\n",
              "DiabetesPedigreeFunction    0\n",
              "Age                         0\n",
              "Outcome                     0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEGlIsedtyKk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "ce117499-c6e9-4953-fa38-ea7ace7906f2"
      },
      "source": [
        "#Convert the data into an array\n",
        "dataset = df.values\n",
        "dataset"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  6.   , 148.   ,  72.   , ...,   0.627,  50.   ,   1.   ],\n",
              "       [  1.   ,  85.   ,  66.   , ...,   0.351,  31.   ,   0.   ],\n",
              "       [  8.   , 183.   ,  64.   , ...,   0.672,  32.   ,   1.   ],\n",
              "       ...,\n",
              "       [  5.   , 121.   ,  72.   , ...,   0.245,  30.   ,   0.   ],\n",
              "       [  1.   , 126.   ,  60.   , ...,   0.349,  47.   ,   1.   ],\n",
              "       [  1.   ,  93.   ,  70.   , ...,   0.315,  23.   ,   0.   ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_sbVYKqt0KS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get all of the rows from the first eight columns of the dataset\n",
        "X = dataset[:,0:8] #X = dataset[:,0:8]   #X = df.iloc[:, 0:8] \n",
        "# Get all of the rows from the last column\n",
        "y = dataset[:,8] #y = dataset[:,8]     #y = df.iloc[:, 8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS43gVBPt2Yb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "d479bb1e-eeff-454e-ecc8-cc1441769a99"
      },
      "source": [
        "#Process the data\n",
        "#the min-max scaler method scales the dataset so that all the input features lie between 0 and 1 inclusive\n",
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(X)\n",
        "X_scale"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.35294118, 0.74371859, 0.59016393, ..., 0.50074516, 0.23441503,\n",
              "        0.48333333],\n",
              "       [0.05882353, 0.42713568, 0.54098361, ..., 0.39642325, 0.11656704,\n",
              "        0.16666667],\n",
              "       [0.47058824, 0.91959799, 0.52459016, ..., 0.34724292, 0.25362938,\n",
              "        0.18333333],\n",
              "       ...,\n",
              "       [0.29411765, 0.6080402 , 0.59016393, ..., 0.390462  , 0.07130658,\n",
              "        0.15      ],\n",
              "       [0.05882353, 0.63316583, 0.49180328, ..., 0.4485842 , 0.11571307,\n",
              "        0.43333333],\n",
              "       [0.05882353, 0.46733668, 0.57377049, ..., 0.45305514, 0.10119556,\n",
              "        0.03333333]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpDBoLfUt4QT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split the data into 80% training and 20%\n",
        "\n",
        "#train_test_split splits arrays or matrices into random train and test subsets. \n",
        "#That means that everytime you run it without specifying random_state, you will get a different result, this is expected behavior.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scale, y, test_size=0.2, random_state = 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC_DQeeAt6Xc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "683f30b1-13ea-4308-bc4f-87cdf069f2f8"
      },
      "source": [
        "#Build the model and architecture of the neural network\n",
        "\n",
        "# The models architechture 3 layers,\n",
        "# 1st layer with 12 neurons and activation function 'relu'\n",
        "# 2nd layer with 15 neurons and activation function 'relu'\n",
        "# the last layer has 1 neuron with an activation function = sigmoid function which returns a value btwn 0 and 1\n",
        "# The input shape/ input_dim = 8 the number of features in the data set\n",
        "model = Sequential([\n",
        "    Dense(12, activation='relu', input_shape=( 8 ,)),\n",
        "    Dense(15, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiPg84S3t-ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "271958ea-6036-476e-c8fd-442c2a48cc51"
      },
      "source": [
        "model.compile(optimizer='sgd', #Stochastic gradient descent optimizer.\n",
        "              loss='binary_crossentropy', #Used for binary classification\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMPMYaN8uFEK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "89ef1694-ef1c-42f7-8ebf-65fe697af1c4"
      },
      "source": [
        "# Split the data into 20% validation data\n",
        "hist = model.fit(X_train, y_train,\n",
        "          batch_size=57, epochs=1000, validation_split=0.2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 491 samples, validate on 123 samples\n",
            "Epoch 1/1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "491/491 [==============================] - 1s 1ms/step - loss: 0.6736 - acc: 0.6477 - val_loss: 0.6726 - val_acc: 0.6504\n",
            "Epoch 2/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6700 - acc: 0.6477 - val_loss: 0.6692 - val_acc: 0.6504\n",
            "Epoch 3/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6667 - acc: 0.6477 - val_loss: 0.6661 - val_acc: 0.6504\n",
            "Epoch 4/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6637 - acc: 0.6477 - val_loss: 0.6631 - val_acc: 0.6504\n",
            "Epoch 5/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6610 - acc: 0.6477 - val_loss: 0.6606 - val_acc: 0.6504\n",
            "Epoch 6/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6585 - acc: 0.6477 - val_loss: 0.6584 - val_acc: 0.6504\n",
            "Epoch 7/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6565 - acc: 0.6477 - val_loss: 0.6565 - val_acc: 0.6504\n",
            "Epoch 8/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6547 - acc: 0.6477 - val_loss: 0.6548 - val_acc: 0.6504\n",
            "Epoch 9/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6530 - acc: 0.6477 - val_loss: 0.6531 - val_acc: 0.6504\n",
            "Epoch 10/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6514 - acc: 0.6477 - val_loss: 0.6517 - val_acc: 0.6504\n",
            "Epoch 11/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6502 - acc: 0.6477 - val_loss: 0.6506 - val_acc: 0.6504\n",
            "Epoch 12/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6491 - acc: 0.6477 - val_loss: 0.6496 - val_acc: 0.6504\n",
            "Epoch 13/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6481 - acc: 0.6477 - val_loss: 0.6487 - val_acc: 0.6504\n",
            "Epoch 14/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6472 - acc: 0.6477 - val_loss: 0.6479 - val_acc: 0.6504\n",
            "Epoch 15/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6464 - acc: 0.6477 - val_loss: 0.6472 - val_acc: 0.6504\n",
            "Epoch 16/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6457 - acc: 0.6477 - val_loss: 0.6465 - val_acc: 0.6504\n",
            "Epoch 17/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6450 - acc: 0.6477 - val_loss: 0.6459 - val_acc: 0.6504\n",
            "Epoch 18/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6445 - acc: 0.6477 - val_loss: 0.6454 - val_acc: 0.6504\n",
            "Epoch 19/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6439 - acc: 0.6477 - val_loss: 0.6449 - val_acc: 0.6504\n",
            "Epoch 20/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6435 - acc: 0.6477 - val_loss: 0.6445 - val_acc: 0.6504\n",
            "Epoch 21/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6430 - acc: 0.6477 - val_loss: 0.6441 - val_acc: 0.6504\n",
            "Epoch 22/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6426 - acc: 0.6477 - val_loss: 0.6438 - val_acc: 0.6504\n",
            "Epoch 23/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6422 - acc: 0.6477 - val_loss: 0.6436 - val_acc: 0.6504\n",
            "Epoch 24/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6419 - acc: 0.6477 - val_loss: 0.6433 - val_acc: 0.6504\n",
            "Epoch 25/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6416 - acc: 0.6477 - val_loss: 0.6431 - val_acc: 0.6504\n",
            "Epoch 26/1000\n",
            "491/491 [==============================] - 0s 59us/step - loss: 0.6414 - acc: 0.6477 - val_loss: 0.6428 - val_acc: 0.6504\n",
            "Epoch 27/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6411 - acc: 0.6477 - val_loss: 0.6426 - val_acc: 0.6504\n",
            "Epoch 28/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6408 - acc: 0.6477 - val_loss: 0.6425 - val_acc: 0.6504\n",
            "Epoch 29/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6406 - acc: 0.6477 - val_loss: 0.6423 - val_acc: 0.6504\n",
            "Epoch 30/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6404 - acc: 0.6477 - val_loss: 0.6421 - val_acc: 0.6504\n",
            "Epoch 31/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6401 - acc: 0.6477 - val_loss: 0.6420 - val_acc: 0.6504\n",
            "Epoch 32/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6399 - acc: 0.6477 - val_loss: 0.6418 - val_acc: 0.6504\n",
            "Epoch 33/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6397 - acc: 0.6477 - val_loss: 0.6417 - val_acc: 0.6504\n",
            "Epoch 34/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6395 - acc: 0.6477 - val_loss: 0.6416 - val_acc: 0.6504\n",
            "Epoch 35/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6394 - acc: 0.6477 - val_loss: 0.6414 - val_acc: 0.6504\n",
            "Epoch 36/1000\n",
            "491/491 [==============================] - 0s 59us/step - loss: 0.6391 - acc: 0.6477 - val_loss: 0.6413 - val_acc: 0.6504\n",
            "Epoch 37/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.6388 - acc: 0.6477 - val_loss: 0.6412 - val_acc: 0.6504\n",
            "Epoch 38/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6387 - acc: 0.6477 - val_loss: 0.6411 - val_acc: 0.6504\n",
            "Epoch 39/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6384 - acc: 0.6477 - val_loss: 0.6410 - val_acc: 0.6504\n",
            "Epoch 40/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6382 - acc: 0.6477 - val_loss: 0.6409 - val_acc: 0.6504\n",
            "Epoch 41/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6381 - acc: 0.6477 - val_loss: 0.6408 - val_acc: 0.6504\n",
            "Epoch 42/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6378 - acc: 0.6477 - val_loss: 0.6407 - val_acc: 0.6504\n",
            "Epoch 43/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6377 - acc: 0.6477 - val_loss: 0.6406 - val_acc: 0.6504\n",
            "Epoch 44/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6374 - acc: 0.6477 - val_loss: 0.6406 - val_acc: 0.6504\n",
            "Epoch 45/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6373 - acc: 0.6477 - val_loss: 0.6405 - val_acc: 0.6504\n",
            "Epoch 46/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6371 - acc: 0.6477 - val_loss: 0.6405 - val_acc: 0.6504\n",
            "Epoch 47/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.6369 - acc: 0.6477 - val_loss: 0.6404 - val_acc: 0.6504\n",
            "Epoch 48/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6367 - acc: 0.6477 - val_loss: 0.6403 - val_acc: 0.6504\n",
            "Epoch 49/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6366 - acc: 0.6477 - val_loss: 0.6402 - val_acc: 0.6504\n",
            "Epoch 50/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6364 - acc: 0.6477 - val_loss: 0.6401 - val_acc: 0.6504\n",
            "Epoch 51/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6362 - acc: 0.6477 - val_loss: 0.6401 - val_acc: 0.6504\n",
            "Epoch 52/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6361 - acc: 0.6477 - val_loss: 0.6400 - val_acc: 0.6504\n",
            "Epoch 53/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6359 - acc: 0.6477 - val_loss: 0.6399 - val_acc: 0.6504\n",
            "Epoch 54/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6357 - acc: 0.6477 - val_loss: 0.6398 - val_acc: 0.6504\n",
            "Epoch 55/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6356 - acc: 0.6477 - val_loss: 0.6397 - val_acc: 0.6504\n",
            "Epoch 56/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6353 - acc: 0.6477 - val_loss: 0.6397 - val_acc: 0.6504\n",
            "Epoch 57/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6352 - acc: 0.6477 - val_loss: 0.6396 - val_acc: 0.6504\n",
            "Epoch 58/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6349 - acc: 0.6477 - val_loss: 0.6396 - val_acc: 0.6504\n",
            "Epoch 59/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6348 - acc: 0.6477 - val_loss: 0.6395 - val_acc: 0.6504\n",
            "Epoch 60/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6347 - acc: 0.6477 - val_loss: 0.6394 - val_acc: 0.6504\n",
            "Epoch 61/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6344 - acc: 0.6477 - val_loss: 0.6394 - val_acc: 0.6504\n",
            "Epoch 62/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6343 - acc: 0.6477 - val_loss: 0.6393 - val_acc: 0.6504\n",
            "Epoch 63/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6341 - acc: 0.6477 - val_loss: 0.6393 - val_acc: 0.6504\n",
            "Epoch 64/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6340 - acc: 0.6477 - val_loss: 0.6392 - val_acc: 0.6504\n",
            "Epoch 65/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6339 - acc: 0.6477 - val_loss: 0.6391 - val_acc: 0.6504\n",
            "Epoch 66/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6337 - acc: 0.6477 - val_loss: 0.6390 - val_acc: 0.6504\n",
            "Epoch 67/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6334 - acc: 0.6477 - val_loss: 0.6390 - val_acc: 0.6504\n",
            "Epoch 68/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6332 - acc: 0.6477 - val_loss: 0.6390 - val_acc: 0.6504\n",
            "Epoch 69/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6331 - acc: 0.6477 - val_loss: 0.6389 - val_acc: 0.6504\n",
            "Epoch 70/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6328 - acc: 0.6477 - val_loss: 0.6389 - val_acc: 0.6504\n",
            "Epoch 71/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.6327 - acc: 0.6477 - val_loss: 0.6388 - val_acc: 0.6504\n",
            "Epoch 72/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6325 - acc: 0.6477 - val_loss: 0.6387 - val_acc: 0.6504\n",
            "Epoch 73/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6323 - acc: 0.6477 - val_loss: 0.6386 - val_acc: 0.6504\n",
            "Epoch 74/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6322 - acc: 0.6477 - val_loss: 0.6385 - val_acc: 0.6504\n",
            "Epoch 75/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6321 - acc: 0.6477 - val_loss: 0.6385 - val_acc: 0.6504\n",
            "Epoch 76/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6319 - acc: 0.6477 - val_loss: 0.6383 - val_acc: 0.6504\n",
            "Epoch 77/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6316 - acc: 0.6477 - val_loss: 0.6382 - val_acc: 0.6504\n",
            "Epoch 78/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.6315 - acc: 0.6477 - val_loss: 0.6381 - val_acc: 0.6504\n",
            "Epoch 79/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6313 - acc: 0.6477 - val_loss: 0.6380 - val_acc: 0.6504\n",
            "Epoch 80/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6312 - acc: 0.6477 - val_loss: 0.6379 - val_acc: 0.6504\n",
            "Epoch 81/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6310 - acc: 0.6477 - val_loss: 0.6378 - val_acc: 0.6504\n",
            "Epoch 82/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6309 - acc: 0.6477 - val_loss: 0.6377 - val_acc: 0.6504\n",
            "Epoch 83/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6307 - acc: 0.6477 - val_loss: 0.6376 - val_acc: 0.6504\n",
            "Epoch 84/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6305 - acc: 0.6477 - val_loss: 0.6374 - val_acc: 0.6504\n",
            "Epoch 85/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6303 - acc: 0.6477 - val_loss: 0.6373 - val_acc: 0.6504\n",
            "Epoch 86/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.6301 - acc: 0.6477 - val_loss: 0.6372 - val_acc: 0.6504\n",
            "Epoch 87/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6300 - acc: 0.6477 - val_loss: 0.6370 - val_acc: 0.6504\n",
            "Epoch 88/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6299 - acc: 0.6477 - val_loss: 0.6369 - val_acc: 0.6504\n",
            "Epoch 89/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6296 - acc: 0.6477 - val_loss: 0.6368 - val_acc: 0.6504\n",
            "Epoch 90/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6294 - acc: 0.6477 - val_loss: 0.6367 - val_acc: 0.6504\n",
            "Epoch 91/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6293 - acc: 0.6477 - val_loss: 0.6365 - val_acc: 0.6504\n",
            "Epoch 92/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6291 - acc: 0.6477 - val_loss: 0.6364 - val_acc: 0.6504\n",
            "Epoch 93/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.6290 - acc: 0.6477 - val_loss: 0.6363 - val_acc: 0.6504\n",
            "Epoch 94/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6287 - acc: 0.6477 - val_loss: 0.6361 - val_acc: 0.6504\n",
            "Epoch 95/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6286 - acc: 0.6477 - val_loss: 0.6360 - val_acc: 0.6504\n",
            "Epoch 96/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6284 - acc: 0.6477 - val_loss: 0.6359 - val_acc: 0.6504\n",
            "Epoch 97/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6283 - acc: 0.6477 - val_loss: 0.6358 - val_acc: 0.6504\n",
            "Epoch 98/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6280 - acc: 0.6477 - val_loss: 0.6356 - val_acc: 0.6504\n",
            "Epoch 99/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6279 - acc: 0.6477 - val_loss: 0.6355 - val_acc: 0.6504\n",
            "Epoch 100/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6276 - acc: 0.6477 - val_loss: 0.6354 - val_acc: 0.6504\n",
            "Epoch 101/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6275 - acc: 0.6477 - val_loss: 0.6352 - val_acc: 0.6504\n",
            "Epoch 102/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6273 - acc: 0.6477 - val_loss: 0.6351 - val_acc: 0.6504\n",
            "Epoch 103/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6271 - acc: 0.6477 - val_loss: 0.6349 - val_acc: 0.6504\n",
            "Epoch 104/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6269 - acc: 0.6477 - val_loss: 0.6348 - val_acc: 0.6504\n",
            "Epoch 105/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6267 - acc: 0.6477 - val_loss: 0.6347 - val_acc: 0.6504\n",
            "Epoch 106/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6266 - acc: 0.6477 - val_loss: 0.6345 - val_acc: 0.6504\n",
            "Epoch 107/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6264 - acc: 0.6477 - val_loss: 0.6344 - val_acc: 0.6504\n",
            "Epoch 108/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6262 - acc: 0.6477 - val_loss: 0.6343 - val_acc: 0.6504\n",
            "Epoch 109/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6260 - acc: 0.6477 - val_loss: 0.6341 - val_acc: 0.6504\n",
            "Epoch 110/1000\n",
            "491/491 [==============================] - 0s 48us/step - loss: 0.6258 - acc: 0.6477 - val_loss: 0.6340 - val_acc: 0.6504\n",
            "Epoch 111/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6256 - acc: 0.6477 - val_loss: 0.6338 - val_acc: 0.6504\n",
            "Epoch 112/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6254 - acc: 0.6477 - val_loss: 0.6337 - val_acc: 0.6504\n",
            "Epoch 113/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6252 - acc: 0.6477 - val_loss: 0.6335 - val_acc: 0.6504\n",
            "Epoch 114/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6250 - acc: 0.6477 - val_loss: 0.6334 - val_acc: 0.6504\n",
            "Epoch 115/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6248 - acc: 0.6477 - val_loss: 0.6332 - val_acc: 0.6504\n",
            "Epoch 116/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6246 - acc: 0.6477 - val_loss: 0.6331 - val_acc: 0.6504\n",
            "Epoch 117/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6245 - acc: 0.6477 - val_loss: 0.6329 - val_acc: 0.6504\n",
            "Epoch 118/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6242 - acc: 0.6477 - val_loss: 0.6328 - val_acc: 0.6504\n",
            "Epoch 119/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6240 - acc: 0.6477 - val_loss: 0.6326 - val_acc: 0.6504\n",
            "Epoch 120/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6238 - acc: 0.6477 - val_loss: 0.6324 - val_acc: 0.6504\n",
            "Epoch 121/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6237 - acc: 0.6477 - val_loss: 0.6323 - val_acc: 0.6504\n",
            "Epoch 122/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6235 - acc: 0.6477 - val_loss: 0.6321 - val_acc: 0.6504\n",
            "Epoch 123/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6232 - acc: 0.6477 - val_loss: 0.6320 - val_acc: 0.6504\n",
            "Epoch 124/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6231 - acc: 0.6477 - val_loss: 0.6318 - val_acc: 0.6504\n",
            "Epoch 125/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6228 - acc: 0.6477 - val_loss: 0.6317 - val_acc: 0.6504\n",
            "Epoch 126/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6226 - acc: 0.6477 - val_loss: 0.6315 - val_acc: 0.6504\n",
            "Epoch 127/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6224 - acc: 0.6477 - val_loss: 0.6313 - val_acc: 0.6504\n",
            "Epoch 128/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6223 - acc: 0.6477 - val_loss: 0.6312 - val_acc: 0.6504\n",
            "Epoch 129/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6220 - acc: 0.6477 - val_loss: 0.6310 - val_acc: 0.6504\n",
            "Epoch 130/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6219 - acc: 0.6477 - val_loss: 0.6309 - val_acc: 0.6504\n",
            "Epoch 131/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6218 - acc: 0.6477 - val_loss: 0.6307 - val_acc: 0.6504\n",
            "Epoch 132/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6214 - acc: 0.6477 - val_loss: 0.6305 - val_acc: 0.6504\n",
            "Epoch 133/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6212 - acc: 0.6477 - val_loss: 0.6304 - val_acc: 0.6504\n",
            "Epoch 134/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6210 - acc: 0.6477 - val_loss: 0.6302 - val_acc: 0.6504\n",
            "Epoch 135/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6209 - acc: 0.6477 - val_loss: 0.6300 - val_acc: 0.6504\n",
            "Epoch 136/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6205 - acc: 0.6477 - val_loss: 0.6299 - val_acc: 0.6504\n",
            "Epoch 137/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6204 - acc: 0.6477 - val_loss: 0.6297 - val_acc: 0.6504\n",
            "Epoch 138/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6203 - acc: 0.6477 - val_loss: 0.6295 - val_acc: 0.6504\n",
            "Epoch 139/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6199 - acc: 0.6477 - val_loss: 0.6294 - val_acc: 0.6504\n",
            "Epoch 140/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6197 - acc: 0.6477 - val_loss: 0.6292 - val_acc: 0.6504\n",
            "Epoch 141/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6194 - acc: 0.6477 - val_loss: 0.6290 - val_acc: 0.6504\n",
            "Epoch 142/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6193 - acc: 0.6477 - val_loss: 0.6289 - val_acc: 0.6504\n",
            "Epoch 143/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6191 - acc: 0.6477 - val_loss: 0.6287 - val_acc: 0.6504\n",
            "Epoch 144/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6189 - acc: 0.6477 - val_loss: 0.6285 - val_acc: 0.6504\n",
            "Epoch 145/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6187 - acc: 0.6477 - val_loss: 0.6284 - val_acc: 0.6504\n",
            "Epoch 146/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6185 - acc: 0.6477 - val_loss: 0.6282 - val_acc: 0.6504\n",
            "Epoch 147/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6183 - acc: 0.6477 - val_loss: 0.6280 - val_acc: 0.6504\n",
            "Epoch 148/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6180 - acc: 0.6477 - val_loss: 0.6279 - val_acc: 0.6504\n",
            "Epoch 149/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6178 - acc: 0.6477 - val_loss: 0.6277 - val_acc: 0.6504\n",
            "Epoch 150/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6177 - acc: 0.6477 - val_loss: 0.6275 - val_acc: 0.6504\n",
            "Epoch 151/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6173 - acc: 0.6477 - val_loss: 0.6274 - val_acc: 0.6504\n",
            "Epoch 152/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6171 - acc: 0.6477 - val_loss: 0.6272 - val_acc: 0.6504\n",
            "Epoch 153/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.6169 - acc: 0.6477 - val_loss: 0.6270 - val_acc: 0.6504\n",
            "Epoch 154/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6167 - acc: 0.6477 - val_loss: 0.6268 - val_acc: 0.6504\n",
            "Epoch 155/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6165 - acc: 0.6477 - val_loss: 0.6267 - val_acc: 0.6504\n",
            "Epoch 156/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6164 - acc: 0.6477 - val_loss: 0.6265 - val_acc: 0.6504\n",
            "Epoch 157/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6162 - acc: 0.6477 - val_loss: 0.6263 - val_acc: 0.6504\n",
            "Epoch 158/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.6159 - acc: 0.6477 - val_loss: 0.6262 - val_acc: 0.6504\n",
            "Epoch 159/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6156 - acc: 0.6477 - val_loss: 0.6260 - val_acc: 0.6504\n",
            "Epoch 160/1000\n",
            "491/491 [==============================] - 0s 66us/step - loss: 0.6154 - acc: 0.6477 - val_loss: 0.6258 - val_acc: 0.6504\n",
            "Epoch 161/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6153 - acc: 0.6477 - val_loss: 0.6256 - val_acc: 0.6504\n",
            "Epoch 162/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6150 - acc: 0.6477 - val_loss: 0.6255 - val_acc: 0.6504\n",
            "Epoch 163/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6148 - acc: 0.6477 - val_loss: 0.6253 - val_acc: 0.6504\n",
            "Epoch 164/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6145 - acc: 0.6477 - val_loss: 0.6251 - val_acc: 0.6504\n",
            "Epoch 165/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6143 - acc: 0.6477 - val_loss: 0.6249 - val_acc: 0.6504\n",
            "Epoch 166/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6141 - acc: 0.6477 - val_loss: 0.6247 - val_acc: 0.6504\n",
            "Epoch 167/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6139 - acc: 0.6477 - val_loss: 0.6246 - val_acc: 0.6504\n",
            "Epoch 168/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6137 - acc: 0.6477 - val_loss: 0.6244 - val_acc: 0.6504\n",
            "Epoch 169/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6134 - acc: 0.6477 - val_loss: 0.6242 - val_acc: 0.6504\n",
            "Epoch 170/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6132 - acc: 0.6477 - val_loss: 0.6240 - val_acc: 0.6504\n",
            "Epoch 171/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6129 - acc: 0.6477 - val_loss: 0.6239 - val_acc: 0.6504\n",
            "Epoch 172/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6128 - acc: 0.6477 - val_loss: 0.6237 - val_acc: 0.6504\n",
            "Epoch 173/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.6125 - acc: 0.6477 - val_loss: 0.6235 - val_acc: 0.6504\n",
            "Epoch 174/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6123 - acc: 0.6477 - val_loss: 0.6233 - val_acc: 0.6504\n",
            "Epoch 175/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6120 - acc: 0.6477 - val_loss: 0.6232 - val_acc: 0.6504\n",
            "Epoch 176/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6118 - acc: 0.6477 - val_loss: 0.6230 - val_acc: 0.6504\n",
            "Epoch 177/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6116 - acc: 0.6477 - val_loss: 0.6228 - val_acc: 0.6504\n",
            "Epoch 178/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6113 - acc: 0.6477 - val_loss: 0.6226 - val_acc: 0.6504\n",
            "Epoch 179/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6111 - acc: 0.6477 - val_loss: 0.6224 - val_acc: 0.6504\n",
            "Epoch 180/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6109 - acc: 0.6477 - val_loss: 0.6223 - val_acc: 0.6504\n",
            "Epoch 181/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6107 - acc: 0.6477 - val_loss: 0.6221 - val_acc: 0.6504\n",
            "Epoch 182/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6104 - acc: 0.6477 - val_loss: 0.6219 - val_acc: 0.6504\n",
            "Epoch 183/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6101 - acc: 0.6477 - val_loss: 0.6217 - val_acc: 0.6504\n",
            "Epoch 184/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6099 - acc: 0.6477 - val_loss: 0.6215 - val_acc: 0.6504\n",
            "Epoch 185/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6096 - acc: 0.6477 - val_loss: 0.6213 - val_acc: 0.6504\n",
            "Epoch 186/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6095 - acc: 0.6477 - val_loss: 0.6212 - val_acc: 0.6504\n",
            "Epoch 187/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6092 - acc: 0.6477 - val_loss: 0.6210 - val_acc: 0.6504\n",
            "Epoch 188/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6089 - acc: 0.6477 - val_loss: 0.6208 - val_acc: 0.6504\n",
            "Epoch 189/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6086 - acc: 0.6477 - val_loss: 0.6206 - val_acc: 0.6504\n",
            "Epoch 190/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6084 - acc: 0.6477 - val_loss: 0.6204 - val_acc: 0.6504\n",
            "Epoch 191/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6082 - acc: 0.6477 - val_loss: 0.6202 - val_acc: 0.6504\n",
            "Epoch 192/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6080 - acc: 0.6477 - val_loss: 0.6201 - val_acc: 0.6504\n",
            "Epoch 193/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6077 - acc: 0.6477 - val_loss: 0.6199 - val_acc: 0.6504\n",
            "Epoch 194/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6075 - acc: 0.6477 - val_loss: 0.6197 - val_acc: 0.6504\n",
            "Epoch 195/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.6072 - acc: 0.6477 - val_loss: 0.6195 - val_acc: 0.6504\n",
            "Epoch 196/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6069 - acc: 0.6477 - val_loss: 0.6193 - val_acc: 0.6504\n",
            "Epoch 197/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6067 - acc: 0.6477 - val_loss: 0.6191 - val_acc: 0.6504\n",
            "Epoch 198/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6065 - acc: 0.6477 - val_loss: 0.6190 - val_acc: 0.6504\n",
            "Epoch 199/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6061 - acc: 0.6477 - val_loss: 0.6188 - val_acc: 0.6504\n",
            "Epoch 200/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6060 - acc: 0.6477 - val_loss: 0.6186 - val_acc: 0.6504\n",
            "Epoch 201/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6058 - acc: 0.6477 - val_loss: 0.6184 - val_acc: 0.6504\n",
            "Epoch 202/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.6055 - acc: 0.6497 - val_loss: 0.6182 - val_acc: 0.6504\n",
            "Epoch 203/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.6054 - acc: 0.6517 - val_loss: 0.6181 - val_acc: 0.6341\n",
            "Epoch 204/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6050 - acc: 0.6497 - val_loss: 0.6179 - val_acc: 0.6341\n",
            "Epoch 205/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6048 - acc: 0.6477 - val_loss: 0.6176 - val_acc: 0.6341\n",
            "Epoch 206/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6045 - acc: 0.6497 - val_loss: 0.6174 - val_acc: 0.6341\n",
            "Epoch 207/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6041 - acc: 0.6497 - val_loss: 0.6172 - val_acc: 0.6341\n",
            "Epoch 208/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6039 - acc: 0.6497 - val_loss: 0.6170 - val_acc: 0.6341\n",
            "Epoch 209/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6036 - acc: 0.6538 - val_loss: 0.6168 - val_acc: 0.6341\n",
            "Epoch 210/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6034 - acc: 0.6558 - val_loss: 0.6167 - val_acc: 0.6341\n",
            "Epoch 211/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6031 - acc: 0.6558 - val_loss: 0.6164 - val_acc: 0.6341\n",
            "Epoch 212/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.6028 - acc: 0.6599 - val_loss: 0.6162 - val_acc: 0.6341\n",
            "Epoch 213/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6025 - acc: 0.6599 - val_loss: 0.6160 - val_acc: 0.6260\n",
            "Epoch 214/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.6022 - acc: 0.6640 - val_loss: 0.6158 - val_acc: 0.6260\n",
            "Epoch 215/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.6020 - acc: 0.6619 - val_loss: 0.6156 - val_acc: 0.6260\n",
            "Epoch 216/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6018 - acc: 0.6640 - val_loss: 0.6154 - val_acc: 0.6260\n",
            "Epoch 217/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6015 - acc: 0.6660 - val_loss: 0.6152 - val_acc: 0.6260\n",
            "Epoch 218/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.6013 - acc: 0.6660 - val_loss: 0.6150 - val_acc: 0.6260\n",
            "Epoch 219/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.6008 - acc: 0.6701 - val_loss: 0.6148 - val_acc: 0.6341\n",
            "Epoch 220/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.6006 - acc: 0.6741 - val_loss: 0.6146 - val_acc: 0.6423\n",
            "Epoch 221/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6003 - acc: 0.6802 - val_loss: 0.6143 - val_acc: 0.6423\n",
            "Epoch 222/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.6001 - acc: 0.6782 - val_loss: 0.6142 - val_acc: 0.6585\n",
            "Epoch 223/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5998 - acc: 0.6762 - val_loss: 0.6139 - val_acc: 0.6585\n",
            "Epoch 224/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5995 - acc: 0.6741 - val_loss: 0.6137 - val_acc: 0.6585\n",
            "Epoch 225/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5991 - acc: 0.6762 - val_loss: 0.6135 - val_acc: 0.6585\n",
            "Epoch 226/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5989 - acc: 0.6802 - val_loss: 0.6133 - val_acc: 0.6585\n",
            "Epoch 227/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5986 - acc: 0.6802 - val_loss: 0.6130 - val_acc: 0.6748\n",
            "Epoch 228/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5982 - acc: 0.6802 - val_loss: 0.6128 - val_acc: 0.6748\n",
            "Epoch 229/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5980 - acc: 0.6802 - val_loss: 0.6126 - val_acc: 0.6748\n",
            "Epoch 230/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5977 - acc: 0.6823 - val_loss: 0.6123 - val_acc: 0.6748\n",
            "Epoch 231/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5973 - acc: 0.6823 - val_loss: 0.6121 - val_acc: 0.6748\n",
            "Epoch 232/1000\n",
            "491/491 [==============================] - 0s 28us/step - loss: 0.5971 - acc: 0.6802 - val_loss: 0.6119 - val_acc: 0.6748\n",
            "Epoch 233/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5968 - acc: 0.6823 - val_loss: 0.6116 - val_acc: 0.6748\n",
            "Epoch 234/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5965 - acc: 0.6843 - val_loss: 0.6114 - val_acc: 0.6748\n",
            "Epoch 235/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5963 - acc: 0.6864 - val_loss: 0.6111 - val_acc: 0.6748\n",
            "Epoch 236/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5959 - acc: 0.6843 - val_loss: 0.6109 - val_acc: 0.6748\n",
            "Epoch 237/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5956 - acc: 0.6802 - val_loss: 0.6107 - val_acc: 0.6748\n",
            "Epoch 238/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5955 - acc: 0.6823 - val_loss: 0.6104 - val_acc: 0.6748\n",
            "Epoch 239/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5949 - acc: 0.6823 - val_loss: 0.6102 - val_acc: 0.6748\n",
            "Epoch 240/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5947 - acc: 0.6843 - val_loss: 0.6099 - val_acc: 0.6585\n",
            "Epoch 241/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5945 - acc: 0.6782 - val_loss: 0.6097 - val_acc: 0.6667\n",
            "Epoch 242/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5941 - acc: 0.6802 - val_loss: 0.6095 - val_acc: 0.6667\n",
            "Epoch 243/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5938 - acc: 0.6864 - val_loss: 0.6092 - val_acc: 0.6667\n",
            "Epoch 244/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5935 - acc: 0.6864 - val_loss: 0.6090 - val_acc: 0.6667\n",
            "Epoch 245/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5932 - acc: 0.6843 - val_loss: 0.6088 - val_acc: 0.6667\n",
            "Epoch 246/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5929 - acc: 0.6843 - val_loss: 0.6086 - val_acc: 0.6667\n",
            "Epoch 247/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5925 - acc: 0.6823 - val_loss: 0.6083 - val_acc: 0.6667\n",
            "Epoch 248/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5924 - acc: 0.6823 - val_loss: 0.6080 - val_acc: 0.6667\n",
            "Epoch 249/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5920 - acc: 0.6843 - val_loss: 0.6078 - val_acc: 0.6667\n",
            "Epoch 250/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5920 - acc: 0.6843 - val_loss: 0.6075 - val_acc: 0.6667\n",
            "Epoch 251/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5913 - acc: 0.6843 - val_loss: 0.6073 - val_acc: 0.6667\n",
            "Epoch 252/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5911 - acc: 0.6864 - val_loss: 0.6071 - val_acc: 0.6667\n",
            "Epoch 253/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5908 - acc: 0.6843 - val_loss: 0.6068 - val_acc: 0.6667\n",
            "Epoch 254/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5904 - acc: 0.6864 - val_loss: 0.6065 - val_acc: 0.6667\n",
            "Epoch 255/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5900 - acc: 0.6843 - val_loss: 0.6063 - val_acc: 0.6667\n",
            "Epoch 256/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5898 - acc: 0.6884 - val_loss: 0.6060 - val_acc: 0.6667\n",
            "Epoch 257/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5893 - acc: 0.6864 - val_loss: 0.6058 - val_acc: 0.6667\n",
            "Epoch 258/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5891 - acc: 0.6884 - val_loss: 0.6055 - val_acc: 0.6667\n",
            "Epoch 259/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5888 - acc: 0.6884 - val_loss: 0.6053 - val_acc: 0.6667\n",
            "Epoch 260/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5888 - acc: 0.6884 - val_loss: 0.6050 - val_acc: 0.6667\n",
            "Epoch 261/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5883 - acc: 0.6864 - val_loss: 0.6048 - val_acc: 0.6667\n",
            "Epoch 262/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5878 - acc: 0.6904 - val_loss: 0.6045 - val_acc: 0.6667\n",
            "Epoch 263/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5877 - acc: 0.6925 - val_loss: 0.6043 - val_acc: 0.6667\n",
            "Epoch 264/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5872 - acc: 0.6904 - val_loss: 0.6040 - val_acc: 0.6667\n",
            "Epoch 265/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5869 - acc: 0.6884 - val_loss: 0.6038 - val_acc: 0.6667\n",
            "Epoch 266/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5871 - acc: 0.6864 - val_loss: 0.6036 - val_acc: 0.6667\n",
            "Epoch 267/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5863 - acc: 0.6904 - val_loss: 0.6033 - val_acc: 0.6667\n",
            "Epoch 268/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5863 - acc: 0.6904 - val_loss: 0.6031 - val_acc: 0.6585\n",
            "Epoch 269/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5858 - acc: 0.6884 - val_loss: 0.6028 - val_acc: 0.6667\n",
            "Epoch 270/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5854 - acc: 0.6904 - val_loss: 0.6026 - val_acc: 0.6667\n",
            "Epoch 271/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5850 - acc: 0.6904 - val_loss: 0.6023 - val_acc: 0.6667\n",
            "Epoch 272/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5848 - acc: 0.6925 - val_loss: 0.6020 - val_acc: 0.6667\n",
            "Epoch 273/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5845 - acc: 0.6945 - val_loss: 0.6017 - val_acc: 0.6667\n",
            "Epoch 274/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5841 - acc: 0.6925 - val_loss: 0.6015 - val_acc: 0.6667\n",
            "Epoch 275/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5837 - acc: 0.6925 - val_loss: 0.6012 - val_acc: 0.6667\n",
            "Epoch 276/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5834 - acc: 0.6925 - val_loss: 0.6010 - val_acc: 0.6667\n",
            "Epoch 277/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5830 - acc: 0.6925 - val_loss: 0.6007 - val_acc: 0.6667\n",
            "Epoch 278/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5827 - acc: 0.6925 - val_loss: 0.6004 - val_acc: 0.6585\n",
            "Epoch 279/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5825 - acc: 0.6945 - val_loss: 0.6002 - val_acc: 0.6667\n",
            "Epoch 280/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5821 - acc: 0.6925 - val_loss: 0.6000 - val_acc: 0.6748\n",
            "Epoch 281/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5818 - acc: 0.7006 - val_loss: 0.5997 - val_acc: 0.6748\n",
            "Epoch 282/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5815 - acc: 0.6986 - val_loss: 0.5994 - val_acc: 0.6748\n",
            "Epoch 283/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5813 - acc: 0.7006 - val_loss: 0.5992 - val_acc: 0.6748\n",
            "Epoch 284/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5807 - acc: 0.6965 - val_loss: 0.5989 - val_acc: 0.6748\n",
            "Epoch 285/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5804 - acc: 0.6986 - val_loss: 0.5987 - val_acc: 0.6667\n",
            "Epoch 286/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5802 - acc: 0.7006 - val_loss: 0.5984 - val_acc: 0.6585\n",
            "Epoch 287/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5802 - acc: 0.7026 - val_loss: 0.5982 - val_acc: 0.6585\n",
            "Epoch 288/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5795 - acc: 0.7026 - val_loss: 0.5979 - val_acc: 0.6585\n",
            "Epoch 289/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5792 - acc: 0.7006 - val_loss: 0.5977 - val_acc: 0.6504\n",
            "Epoch 290/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5787 - acc: 0.7006 - val_loss: 0.5974 - val_acc: 0.6585\n",
            "Epoch 291/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5783 - acc: 0.7026 - val_loss: 0.5972 - val_acc: 0.6585\n",
            "Epoch 292/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5782 - acc: 0.7047 - val_loss: 0.5970 - val_acc: 0.6585\n",
            "Epoch 293/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.5777 - acc: 0.7026 - val_loss: 0.5967 - val_acc: 0.6585\n",
            "Epoch 294/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5774 - acc: 0.7026 - val_loss: 0.5964 - val_acc: 0.6585\n",
            "Epoch 295/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5770 - acc: 0.7026 - val_loss: 0.5962 - val_acc: 0.6585\n",
            "Epoch 296/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5767 - acc: 0.7006 - val_loss: 0.5959 - val_acc: 0.6585\n",
            "Epoch 297/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5763 - acc: 0.7006 - val_loss: 0.5957 - val_acc: 0.6585\n",
            "Epoch 298/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5764 - acc: 0.7006 - val_loss: 0.5954 - val_acc: 0.6585\n",
            "Epoch 299/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5757 - acc: 0.6986 - val_loss: 0.5952 - val_acc: 0.6585\n",
            "Epoch 300/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5753 - acc: 0.7006 - val_loss: 0.5949 - val_acc: 0.6585\n",
            "Epoch 301/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5749 - acc: 0.6986 - val_loss: 0.5947 - val_acc: 0.6585\n",
            "Epoch 302/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5746 - acc: 0.6986 - val_loss: 0.5944 - val_acc: 0.6585\n",
            "Epoch 303/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5743 - acc: 0.6986 - val_loss: 0.5941 - val_acc: 0.6585\n",
            "Epoch 304/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5741 - acc: 0.6986 - val_loss: 0.5939 - val_acc: 0.6585\n",
            "Epoch 305/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5737 - acc: 0.6986 - val_loss: 0.5936 - val_acc: 0.6585\n",
            "Epoch 306/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5732 - acc: 0.6986 - val_loss: 0.5934 - val_acc: 0.6585\n",
            "Epoch 307/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5728 - acc: 0.6986 - val_loss: 0.5932 - val_acc: 0.6585\n",
            "Epoch 308/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5725 - acc: 0.6986 - val_loss: 0.5929 - val_acc: 0.6667\n",
            "Epoch 309/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5721 - acc: 0.6965 - val_loss: 0.5927 - val_acc: 0.6667\n",
            "Epoch 310/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5718 - acc: 0.7026 - val_loss: 0.5924 - val_acc: 0.6585\n",
            "Epoch 311/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5715 - acc: 0.7006 - val_loss: 0.5922 - val_acc: 0.6585\n",
            "Epoch 312/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5713 - acc: 0.6965 - val_loss: 0.5919 - val_acc: 0.6667\n",
            "Epoch 313/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5707 - acc: 0.6986 - val_loss: 0.5917 - val_acc: 0.6667\n",
            "Epoch 314/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5705 - acc: 0.6986 - val_loss: 0.5914 - val_acc: 0.6585\n",
            "Epoch 315/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5700 - acc: 0.7026 - val_loss: 0.5912 - val_acc: 0.6585\n",
            "Epoch 316/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5696 - acc: 0.7006 - val_loss: 0.5910 - val_acc: 0.6585\n",
            "Epoch 317/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5693 - acc: 0.7026 - val_loss: 0.5907 - val_acc: 0.6585\n",
            "Epoch 318/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5689 - acc: 0.7006 - val_loss: 0.5904 - val_acc: 0.6585\n",
            "Epoch 319/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5685 - acc: 0.7006 - val_loss: 0.5902 - val_acc: 0.6585\n",
            "Epoch 320/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5682 - acc: 0.7006 - val_loss: 0.5900 - val_acc: 0.6585\n",
            "Epoch 321/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5679 - acc: 0.6965 - val_loss: 0.5897 - val_acc: 0.6585\n",
            "Epoch 322/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5675 - acc: 0.7026 - val_loss: 0.5895 - val_acc: 0.6585\n",
            "Epoch 323/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5671 - acc: 0.7067 - val_loss: 0.5892 - val_acc: 0.6504\n",
            "Epoch 324/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.5668 - acc: 0.7006 - val_loss: 0.5890 - val_acc: 0.6504\n",
            "Epoch 325/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5665 - acc: 0.7006 - val_loss: 0.5888 - val_acc: 0.6504\n",
            "Epoch 326/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5660 - acc: 0.7026 - val_loss: 0.5885 - val_acc: 0.6504\n",
            "Epoch 327/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5661 - acc: 0.7026 - val_loss: 0.5883 - val_acc: 0.6423\n",
            "Epoch 328/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5654 - acc: 0.7026 - val_loss: 0.5880 - val_acc: 0.6504\n",
            "Epoch 329/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5649 - acc: 0.7026 - val_loss: 0.5878 - val_acc: 0.6504\n",
            "Epoch 330/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5646 - acc: 0.7026 - val_loss: 0.5875 - val_acc: 0.6504\n",
            "Epoch 331/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.5642 - acc: 0.7047 - val_loss: 0.5873 - val_acc: 0.6504\n",
            "Epoch 332/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5642 - acc: 0.7026 - val_loss: 0.5870 - val_acc: 0.6504\n",
            "Epoch 333/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5636 - acc: 0.7006 - val_loss: 0.5868 - val_acc: 0.6504\n",
            "Epoch 334/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5631 - acc: 0.7047 - val_loss: 0.5865 - val_acc: 0.6504\n",
            "Epoch 335/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5627 - acc: 0.7026 - val_loss: 0.5862 - val_acc: 0.6504\n",
            "Epoch 336/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5623 - acc: 0.7047 - val_loss: 0.5860 - val_acc: 0.6504\n",
            "Epoch 337/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5621 - acc: 0.6986 - val_loss: 0.5857 - val_acc: 0.6504\n",
            "Epoch 338/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5617 - acc: 0.7026 - val_loss: 0.5854 - val_acc: 0.6423\n",
            "Epoch 339/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5615 - acc: 0.7067 - val_loss: 0.5853 - val_acc: 0.6504\n",
            "Epoch 340/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5609 - acc: 0.7047 - val_loss: 0.5850 - val_acc: 0.6504\n",
            "Epoch 341/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5606 - acc: 0.7026 - val_loss: 0.5847 - val_acc: 0.6504\n",
            "Epoch 342/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5601 - acc: 0.6986 - val_loss: 0.5844 - val_acc: 0.6504\n",
            "Epoch 343/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5597 - acc: 0.6986 - val_loss: 0.5842 - val_acc: 0.6504\n",
            "Epoch 344/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5596 - acc: 0.6986 - val_loss: 0.5839 - val_acc: 0.6504\n",
            "Epoch 345/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5590 - acc: 0.7067 - val_loss: 0.5836 - val_acc: 0.6504\n",
            "Epoch 346/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5587 - acc: 0.7006 - val_loss: 0.5834 - val_acc: 0.6504\n",
            "Epoch 347/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5583 - acc: 0.6986 - val_loss: 0.5831 - val_acc: 0.6504\n",
            "Epoch 348/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5579 - acc: 0.6986 - val_loss: 0.5829 - val_acc: 0.6504\n",
            "Epoch 349/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5576 - acc: 0.7047 - val_loss: 0.5826 - val_acc: 0.6504\n",
            "Epoch 350/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5573 - acc: 0.7006 - val_loss: 0.5823 - val_acc: 0.6504\n",
            "Epoch 351/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5569 - acc: 0.7047 - val_loss: 0.5820 - val_acc: 0.6504\n",
            "Epoch 352/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5564 - acc: 0.6965 - val_loss: 0.5818 - val_acc: 0.6504\n",
            "Epoch 353/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5561 - acc: 0.6986 - val_loss: 0.5816 - val_acc: 0.6585\n",
            "Epoch 354/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5557 - acc: 0.6986 - val_loss: 0.5813 - val_acc: 0.6585\n",
            "Epoch 355/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5552 - acc: 0.7047 - val_loss: 0.5811 - val_acc: 0.6585\n",
            "Epoch 356/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5549 - acc: 0.7006 - val_loss: 0.5808 - val_acc: 0.6585\n",
            "Epoch 357/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5545 - acc: 0.7026 - val_loss: 0.5805 - val_acc: 0.6585\n",
            "Epoch 358/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5543 - acc: 0.7006 - val_loss: 0.5802 - val_acc: 0.6585\n",
            "Epoch 359/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5536 - acc: 0.7006 - val_loss: 0.5799 - val_acc: 0.6585\n",
            "Epoch 360/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5533 - acc: 0.7006 - val_loss: 0.5796 - val_acc: 0.6585\n",
            "Epoch 361/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5529 - acc: 0.7067 - val_loss: 0.5794 - val_acc: 0.6585\n",
            "Epoch 362/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5524 - acc: 0.7026 - val_loss: 0.5792 - val_acc: 0.6504\n",
            "Epoch 363/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5522 - acc: 0.7026 - val_loss: 0.5789 - val_acc: 0.6504\n",
            "Epoch 364/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5519 - acc: 0.7047 - val_loss: 0.5786 - val_acc: 0.6504\n",
            "Epoch 365/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5515 - acc: 0.7006 - val_loss: 0.5783 - val_acc: 0.6504\n",
            "Epoch 366/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5510 - acc: 0.7026 - val_loss: 0.5781 - val_acc: 0.6504\n",
            "Epoch 367/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5506 - acc: 0.7047 - val_loss: 0.5778 - val_acc: 0.6504\n",
            "Epoch 368/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5502 - acc: 0.7047 - val_loss: 0.5775 - val_acc: 0.6504\n",
            "Epoch 369/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5498 - acc: 0.7067 - val_loss: 0.5773 - val_acc: 0.6341\n",
            "Epoch 370/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5495 - acc: 0.7108 - val_loss: 0.5770 - val_acc: 0.6423\n",
            "Epoch 371/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5494 - acc: 0.7047 - val_loss: 0.5767 - val_acc: 0.6423\n",
            "Epoch 372/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5488 - acc: 0.7067 - val_loss: 0.5765 - val_acc: 0.6341\n",
            "Epoch 373/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5482 - acc: 0.7088 - val_loss: 0.5762 - val_acc: 0.6341\n",
            "Epoch 374/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5480 - acc: 0.7108 - val_loss: 0.5760 - val_acc: 0.6260\n",
            "Epoch 375/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5474 - acc: 0.7128 - val_loss: 0.5758 - val_acc: 0.6260\n",
            "Epoch 376/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5472 - acc: 0.7026 - val_loss: 0.5755 - val_acc: 0.6260\n",
            "Epoch 377/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5469 - acc: 0.7006 - val_loss: 0.5751 - val_acc: 0.6341\n",
            "Epoch 378/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5463 - acc: 0.7149 - val_loss: 0.5749 - val_acc: 0.6260\n",
            "Epoch 379/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5462 - acc: 0.7026 - val_loss: 0.5747 - val_acc: 0.6179\n",
            "Epoch 380/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5456 - acc: 0.7006 - val_loss: 0.5744 - val_acc: 0.6341\n",
            "Epoch 381/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5451 - acc: 0.7088 - val_loss: 0.5742 - val_acc: 0.6098\n",
            "Epoch 382/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5449 - acc: 0.7026 - val_loss: 0.5739 - val_acc: 0.6179\n",
            "Epoch 383/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5444 - acc: 0.7047 - val_loss: 0.5737 - val_acc: 0.6098\n",
            "Epoch 384/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5441 - acc: 0.7026 - val_loss: 0.5734 - val_acc: 0.6179\n",
            "Epoch 385/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5440 - acc: 0.7047 - val_loss: 0.5732 - val_acc: 0.6098\n",
            "Epoch 386/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5434 - acc: 0.7006 - val_loss: 0.5729 - val_acc: 0.6179\n",
            "Epoch 387/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5433 - acc: 0.7067 - val_loss: 0.5727 - val_acc: 0.6098\n",
            "Epoch 388/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5423 - acc: 0.7026 - val_loss: 0.5724 - val_acc: 0.6098\n",
            "Epoch 389/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5420 - acc: 0.7047 - val_loss: 0.5722 - val_acc: 0.6098\n",
            "Epoch 390/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5416 - acc: 0.7108 - val_loss: 0.5720 - val_acc: 0.6098\n",
            "Epoch 391/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5413 - acc: 0.7088 - val_loss: 0.5717 - val_acc: 0.6098\n",
            "Epoch 392/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5410 - acc: 0.7088 - val_loss: 0.5715 - val_acc: 0.6179\n",
            "Epoch 393/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5407 - acc: 0.7067 - val_loss: 0.5712 - val_acc: 0.6098\n",
            "Epoch 394/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5405 - acc: 0.7108 - val_loss: 0.5709 - val_acc: 0.6098\n",
            "Epoch 395/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.5398 - acc: 0.7067 - val_loss: 0.5708 - val_acc: 0.6179\n",
            "Epoch 396/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5394 - acc: 0.7088 - val_loss: 0.5706 - val_acc: 0.6260\n",
            "Epoch 397/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5390 - acc: 0.7067 - val_loss: 0.5703 - val_acc: 0.6260\n",
            "Epoch 398/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5386 - acc: 0.7006 - val_loss: 0.5700 - val_acc: 0.6260\n",
            "Epoch 399/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5383 - acc: 0.7026 - val_loss: 0.5698 - val_acc: 0.6260\n",
            "Epoch 400/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5378 - acc: 0.7067 - val_loss: 0.5695 - val_acc: 0.6260\n",
            "Epoch 401/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5376 - acc: 0.7128 - val_loss: 0.5693 - val_acc: 0.6260\n",
            "Epoch 402/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5371 - acc: 0.7088 - val_loss: 0.5690 - val_acc: 0.6260\n",
            "Epoch 403/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5369 - acc: 0.7108 - val_loss: 0.5688 - val_acc: 0.6341\n",
            "Epoch 404/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.5367 - acc: 0.7128 - val_loss: 0.5685 - val_acc: 0.6260\n",
            "Epoch 405/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5361 - acc: 0.7128 - val_loss: 0.5683 - val_acc: 0.6260\n",
            "Epoch 406/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5356 - acc: 0.7108 - val_loss: 0.5680 - val_acc: 0.6341\n",
            "Epoch 407/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5355 - acc: 0.7108 - val_loss: 0.5678 - val_acc: 0.6341\n",
            "Epoch 408/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5349 - acc: 0.7128 - val_loss: 0.5675 - val_acc: 0.6341\n",
            "Epoch 409/1000\n",
            "491/491 [==============================] - 0s 28us/step - loss: 0.5345 - acc: 0.7128 - val_loss: 0.5673 - val_acc: 0.6423\n",
            "Epoch 410/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5343 - acc: 0.7128 - val_loss: 0.5671 - val_acc: 0.6423\n",
            "Epoch 411/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5338 - acc: 0.7108 - val_loss: 0.5668 - val_acc: 0.6423\n",
            "Epoch 412/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5334 - acc: 0.7128 - val_loss: 0.5666 - val_acc: 0.6423\n",
            "Epoch 413/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5331 - acc: 0.7149 - val_loss: 0.5664 - val_acc: 0.6423\n",
            "Epoch 414/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.5326 - acc: 0.7169 - val_loss: 0.5662 - val_acc: 0.6423\n",
            "Epoch 415/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5325 - acc: 0.7149 - val_loss: 0.5659 - val_acc: 0.6423\n",
            "Epoch 416/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.5321 - acc: 0.7210 - val_loss: 0.5658 - val_acc: 0.6504\n",
            "Epoch 417/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5317 - acc: 0.7088 - val_loss: 0.5655 - val_acc: 0.6585\n",
            "Epoch 418/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5313 - acc: 0.7169 - val_loss: 0.5653 - val_acc: 0.6504\n",
            "Epoch 419/1000\n",
            "491/491 [==============================] - 0s 52us/step - loss: 0.5307 - acc: 0.7108 - val_loss: 0.5651 - val_acc: 0.6585\n",
            "Epoch 420/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5303 - acc: 0.7169 - val_loss: 0.5648 - val_acc: 0.6585\n",
            "Epoch 421/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5300 - acc: 0.7149 - val_loss: 0.5646 - val_acc: 0.6504\n",
            "Epoch 422/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5297 - acc: 0.7149 - val_loss: 0.5644 - val_acc: 0.6585\n",
            "Epoch 423/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.5293 - acc: 0.7149 - val_loss: 0.5642 - val_acc: 0.6585\n",
            "Epoch 424/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5292 - acc: 0.7169 - val_loss: 0.5640 - val_acc: 0.6504\n",
            "Epoch 425/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5289 - acc: 0.7149 - val_loss: 0.5637 - val_acc: 0.6585\n",
            "Epoch 426/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5281 - acc: 0.7169 - val_loss: 0.5635 - val_acc: 0.6504\n",
            "Epoch 427/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5282 - acc: 0.7169 - val_loss: 0.5633 - val_acc: 0.6585\n",
            "Epoch 428/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5275 - acc: 0.7149 - val_loss: 0.5631 - val_acc: 0.6585\n",
            "Epoch 429/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5271 - acc: 0.7169 - val_loss: 0.5628 - val_acc: 0.6585\n",
            "Epoch 430/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5271 - acc: 0.7128 - val_loss: 0.5626 - val_acc: 0.6585\n",
            "Epoch 431/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.5266 - acc: 0.7210 - val_loss: 0.5624 - val_acc: 0.6585\n",
            "Epoch 432/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5263 - acc: 0.7189 - val_loss: 0.5622 - val_acc: 0.6585\n",
            "Epoch 433/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5262 - acc: 0.7210 - val_loss: 0.5620 - val_acc: 0.6585\n",
            "Epoch 434/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5253 - acc: 0.7189 - val_loss: 0.5618 - val_acc: 0.6585\n",
            "Epoch 435/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5253 - acc: 0.7210 - val_loss: 0.5616 - val_acc: 0.6585\n",
            "Epoch 436/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5250 - acc: 0.7210 - val_loss: 0.5613 - val_acc: 0.6585\n",
            "Epoch 437/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5246 - acc: 0.7210 - val_loss: 0.5611 - val_acc: 0.6585\n",
            "Epoch 438/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5242 - acc: 0.7210 - val_loss: 0.5610 - val_acc: 0.6585\n",
            "Epoch 439/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5244 - acc: 0.7230 - val_loss: 0.5607 - val_acc: 0.6504\n",
            "Epoch 440/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5234 - acc: 0.7230 - val_loss: 0.5605 - val_acc: 0.6585\n",
            "Epoch 441/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.5228 - acc: 0.7189 - val_loss: 0.5603 - val_acc: 0.6585\n",
            "Epoch 442/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5225 - acc: 0.7230 - val_loss: 0.5601 - val_acc: 0.6585\n",
            "Epoch 443/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5223 - acc: 0.7210 - val_loss: 0.5599 - val_acc: 0.6585\n",
            "Epoch 444/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5219 - acc: 0.7230 - val_loss: 0.5597 - val_acc: 0.6585\n",
            "Epoch 445/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.5216 - acc: 0.7189 - val_loss: 0.5595 - val_acc: 0.6585\n",
            "Epoch 446/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5212 - acc: 0.7230 - val_loss: 0.5593 - val_acc: 0.6504\n",
            "Epoch 447/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5209 - acc: 0.7149 - val_loss: 0.5591 - val_acc: 0.6585\n",
            "Epoch 448/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5202 - acc: 0.7210 - val_loss: 0.5589 - val_acc: 0.6585\n",
            "Epoch 449/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5203 - acc: 0.7189 - val_loss: 0.5588 - val_acc: 0.6504\n",
            "Epoch 450/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5197 - acc: 0.7230 - val_loss: 0.5586 - val_acc: 0.6504\n",
            "Epoch 451/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5197 - acc: 0.7251 - val_loss: 0.5584 - val_acc: 0.6504\n",
            "Epoch 452/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5194 - acc: 0.7210 - val_loss: 0.5582 - val_acc: 0.6504\n",
            "Epoch 453/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5187 - acc: 0.7189 - val_loss: 0.5579 - val_acc: 0.6585\n",
            "Epoch 454/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5183 - acc: 0.7230 - val_loss: 0.5578 - val_acc: 0.6585\n",
            "Epoch 455/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5180 - acc: 0.7189 - val_loss: 0.5576 - val_acc: 0.6504\n",
            "Epoch 456/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5174 - acc: 0.7251 - val_loss: 0.5574 - val_acc: 0.6585\n",
            "Epoch 457/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.5171 - acc: 0.7230 - val_loss: 0.5572 - val_acc: 0.6585\n",
            "Epoch 458/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.5166 - acc: 0.7251 - val_loss: 0.5570 - val_acc: 0.6585\n",
            "Epoch 459/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5165 - acc: 0.7271 - val_loss: 0.5568 - val_acc: 0.6504\n",
            "Epoch 460/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5159 - acc: 0.7251 - val_loss: 0.5567 - val_acc: 0.6504\n",
            "Epoch 461/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5155 - acc: 0.7251 - val_loss: 0.5565 - val_acc: 0.6585\n",
            "Epoch 462/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5156 - acc: 0.7271 - val_loss: 0.5563 - val_acc: 0.6504\n",
            "Epoch 463/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5152 - acc: 0.7251 - val_loss: 0.5562 - val_acc: 0.6667\n",
            "Epoch 464/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5146 - acc: 0.7251 - val_loss: 0.5560 - val_acc: 0.6667\n",
            "Epoch 465/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5142 - acc: 0.7210 - val_loss: 0.5558 - val_acc: 0.6504\n",
            "Epoch 466/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5139 - acc: 0.7271 - val_loss: 0.5557 - val_acc: 0.6667\n",
            "Epoch 467/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5138 - acc: 0.7291 - val_loss: 0.5555 - val_acc: 0.6667\n",
            "Epoch 468/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5131 - acc: 0.7271 - val_loss: 0.5554 - val_acc: 0.6504\n",
            "Epoch 469/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5129 - acc: 0.7271 - val_loss: 0.5552 - val_acc: 0.6667\n",
            "Epoch 470/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.5127 - acc: 0.7251 - val_loss: 0.5551 - val_acc: 0.6667\n",
            "Epoch 471/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5124 - acc: 0.7251 - val_loss: 0.5550 - val_acc: 0.6585\n",
            "Epoch 472/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5122 - acc: 0.7271 - val_loss: 0.5548 - val_acc: 0.6585\n",
            "Epoch 473/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.5121 - acc: 0.7312 - val_loss: 0.5546 - val_acc: 0.6667\n",
            "Epoch 474/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5113 - acc: 0.7271 - val_loss: 0.5545 - val_acc: 0.6667\n",
            "Epoch 475/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5107 - acc: 0.7271 - val_loss: 0.5543 - val_acc: 0.6667\n",
            "Epoch 476/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5105 - acc: 0.7291 - val_loss: 0.5542 - val_acc: 0.6667\n",
            "Epoch 477/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5102 - acc: 0.7291 - val_loss: 0.5541 - val_acc: 0.6748\n",
            "Epoch 478/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.5102 - acc: 0.7251 - val_loss: 0.5539 - val_acc: 0.6667\n",
            "Epoch 479/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5099 - acc: 0.7271 - val_loss: 0.5538 - val_acc: 0.6667\n",
            "Epoch 480/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5090 - acc: 0.7230 - val_loss: 0.5537 - val_acc: 0.6667\n",
            "Epoch 481/1000\n",
            "491/491 [==============================] - 0s 47us/step - loss: 0.5088 - acc: 0.7189 - val_loss: 0.5535 - val_acc: 0.6667\n",
            "Epoch 482/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5087 - acc: 0.7291 - val_loss: 0.5534 - val_acc: 0.6667\n",
            "Epoch 483/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.5082 - acc: 0.7332 - val_loss: 0.5533 - val_acc: 0.6748\n",
            "Epoch 484/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5079 - acc: 0.7312 - val_loss: 0.5531 - val_acc: 0.6667\n",
            "Epoch 485/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5080 - acc: 0.7312 - val_loss: 0.5530 - val_acc: 0.6748\n",
            "Epoch 486/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5075 - acc: 0.7312 - val_loss: 0.5530 - val_acc: 0.6667\n",
            "Epoch 487/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.5070 - acc: 0.7312 - val_loss: 0.5527 - val_acc: 0.6585\n",
            "Epoch 488/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5068 - acc: 0.7352 - val_loss: 0.5526 - val_acc: 0.6585\n",
            "Epoch 489/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5060 - acc: 0.7352 - val_loss: 0.5524 - val_acc: 0.6748\n",
            "Epoch 490/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5059 - acc: 0.7312 - val_loss: 0.5522 - val_acc: 0.6748\n",
            "Epoch 491/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5055 - acc: 0.7352 - val_loss: 0.5521 - val_acc: 0.6748\n",
            "Epoch 492/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5054 - acc: 0.7332 - val_loss: 0.5519 - val_acc: 0.6667\n",
            "Epoch 493/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.5053 - acc: 0.7291 - val_loss: 0.5518 - val_acc: 0.6748\n",
            "Epoch 494/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5046 - acc: 0.7373 - val_loss: 0.5516 - val_acc: 0.6748\n",
            "Epoch 495/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5044 - acc: 0.7332 - val_loss: 0.5516 - val_acc: 0.6748\n",
            "Epoch 496/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5040 - acc: 0.7312 - val_loss: 0.5514 - val_acc: 0.6748\n",
            "Epoch 497/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5036 - acc: 0.7332 - val_loss: 0.5513 - val_acc: 0.6748\n",
            "Epoch 498/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5032 - acc: 0.7332 - val_loss: 0.5510 - val_acc: 0.6748\n",
            "Epoch 499/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5032 - acc: 0.7373 - val_loss: 0.5509 - val_acc: 0.6748\n",
            "Epoch 500/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5027 - acc: 0.7312 - val_loss: 0.5508 - val_acc: 0.6829\n",
            "Epoch 501/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5022 - acc: 0.7373 - val_loss: 0.5507 - val_acc: 0.6748\n",
            "Epoch 502/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5020 - acc: 0.7373 - val_loss: 0.5505 - val_acc: 0.6748\n",
            "Epoch 503/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.5016 - acc: 0.7373 - val_loss: 0.5504 - val_acc: 0.6911\n",
            "Epoch 504/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5012 - acc: 0.7352 - val_loss: 0.5502 - val_acc: 0.6911\n",
            "Epoch 505/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.5014 - acc: 0.7393 - val_loss: 0.5501 - val_acc: 0.6911\n",
            "Epoch 506/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.5010 - acc: 0.7312 - val_loss: 0.5500 - val_acc: 0.6911\n",
            "Epoch 507/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.5005 - acc: 0.7373 - val_loss: 0.5498 - val_acc: 0.6748\n",
            "Epoch 508/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.5000 - acc: 0.7434 - val_loss: 0.5497 - val_acc: 0.6829\n",
            "Epoch 509/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4999 - acc: 0.7352 - val_loss: 0.5496 - val_acc: 0.6829\n",
            "Epoch 510/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4995 - acc: 0.7373 - val_loss: 0.5494 - val_acc: 0.6829\n",
            "Epoch 511/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4991 - acc: 0.7413 - val_loss: 0.5493 - val_acc: 0.6829\n",
            "Epoch 512/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4991 - acc: 0.7373 - val_loss: 0.5492 - val_acc: 0.6748\n",
            "Epoch 513/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4987 - acc: 0.7454 - val_loss: 0.5490 - val_acc: 0.6829\n",
            "Epoch 514/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4986 - acc: 0.7393 - val_loss: 0.5489 - val_acc: 0.6748\n",
            "Epoch 515/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4981 - acc: 0.7413 - val_loss: 0.5488 - val_acc: 0.6829\n",
            "Epoch 516/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4982 - acc: 0.7393 - val_loss: 0.5487 - val_acc: 0.6667\n",
            "Epoch 517/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4978 - acc: 0.7332 - val_loss: 0.5485 - val_acc: 0.6829\n",
            "Epoch 518/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4976 - acc: 0.7312 - val_loss: 0.5484 - val_acc: 0.6748\n",
            "Epoch 519/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4968 - acc: 0.7393 - val_loss: 0.5483 - val_acc: 0.6829\n",
            "Epoch 520/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4966 - acc: 0.7454 - val_loss: 0.5482 - val_acc: 0.6829\n",
            "Epoch 521/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4966 - acc: 0.7413 - val_loss: 0.5481 - val_acc: 0.6829\n",
            "Epoch 522/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4959 - acc: 0.7434 - val_loss: 0.5479 - val_acc: 0.6829\n",
            "Epoch 523/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4963 - acc: 0.7454 - val_loss: 0.5478 - val_acc: 0.6829\n",
            "Epoch 524/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4957 - acc: 0.7393 - val_loss: 0.5477 - val_acc: 0.6829\n",
            "Epoch 525/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4950 - acc: 0.7413 - val_loss: 0.5476 - val_acc: 0.6829\n",
            "Epoch 526/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4949 - acc: 0.7454 - val_loss: 0.5475 - val_acc: 0.6748\n",
            "Epoch 527/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4953 - acc: 0.7413 - val_loss: 0.5475 - val_acc: 0.6748\n",
            "Epoch 528/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4949 - acc: 0.7373 - val_loss: 0.5473 - val_acc: 0.6748\n",
            "Epoch 529/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4940 - acc: 0.7454 - val_loss: 0.5472 - val_acc: 0.6829\n",
            "Epoch 530/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4941 - acc: 0.7495 - val_loss: 0.5471 - val_acc: 0.6829\n",
            "Epoch 531/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4935 - acc: 0.7495 - val_loss: 0.5470 - val_acc: 0.6829\n",
            "Epoch 532/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4933 - acc: 0.7475 - val_loss: 0.5470 - val_acc: 0.6748\n",
            "Epoch 533/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4936 - acc: 0.7454 - val_loss: 0.5470 - val_acc: 0.6667\n",
            "Epoch 534/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4930 - acc: 0.7413 - val_loss: 0.5467 - val_acc: 0.6829\n",
            "Epoch 535/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4930 - acc: 0.7475 - val_loss: 0.5468 - val_acc: 0.6748\n",
            "Epoch 536/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4927 - acc: 0.7373 - val_loss: 0.5465 - val_acc: 0.6748\n",
            "Epoch 537/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4922 - acc: 0.7413 - val_loss: 0.5464 - val_acc: 0.6829\n",
            "Epoch 538/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4918 - acc: 0.7495 - val_loss: 0.5463 - val_acc: 0.6829\n",
            "Epoch 539/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4913 - acc: 0.7475 - val_loss: 0.5463 - val_acc: 0.6748\n",
            "Epoch 540/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4912 - acc: 0.7434 - val_loss: 0.5461 - val_acc: 0.6829\n",
            "Epoch 541/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4913 - acc: 0.7515 - val_loss: 0.5460 - val_acc: 0.6829\n",
            "Epoch 542/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4907 - acc: 0.7515 - val_loss: 0.5460 - val_acc: 0.6748\n",
            "Epoch 543/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4902 - acc: 0.7454 - val_loss: 0.5459 - val_acc: 0.6829\n",
            "Epoch 544/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4900 - acc: 0.7475 - val_loss: 0.5458 - val_acc: 0.6748\n",
            "Epoch 545/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4901 - acc: 0.7393 - val_loss: 0.5457 - val_acc: 0.6829\n",
            "Epoch 546/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4897 - acc: 0.7495 - val_loss: 0.5457 - val_acc: 0.6748\n",
            "Epoch 547/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4897 - acc: 0.7475 - val_loss: 0.5459 - val_acc: 0.6748\n",
            "Epoch 548/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4893 - acc: 0.7393 - val_loss: 0.5455 - val_acc: 0.6748\n",
            "Epoch 549/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4899 - acc: 0.7373 - val_loss: 0.5453 - val_acc: 0.6748\n",
            "Epoch 550/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4889 - acc: 0.7454 - val_loss: 0.5452 - val_acc: 0.6829\n",
            "Epoch 551/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4888 - acc: 0.7454 - val_loss: 0.5452 - val_acc: 0.6748\n",
            "Epoch 552/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4884 - acc: 0.7454 - val_loss: 0.5450 - val_acc: 0.6829\n",
            "Epoch 553/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4879 - acc: 0.7515 - val_loss: 0.5450 - val_acc: 0.6829\n",
            "Epoch 554/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4884 - acc: 0.7475 - val_loss: 0.5449 - val_acc: 0.6829\n",
            "Epoch 555/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4877 - acc: 0.7454 - val_loss: 0.5448 - val_acc: 0.6829\n",
            "Epoch 556/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4876 - acc: 0.7475 - val_loss: 0.5448 - val_acc: 0.6829\n",
            "Epoch 557/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4875 - acc: 0.7454 - val_loss: 0.5448 - val_acc: 0.6829\n",
            "Epoch 558/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4877 - acc: 0.7454 - val_loss: 0.5446 - val_acc: 0.6829\n",
            "Epoch 559/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4864 - acc: 0.7495 - val_loss: 0.5445 - val_acc: 0.6829\n",
            "Epoch 560/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4864 - acc: 0.7454 - val_loss: 0.5444 - val_acc: 0.6829\n",
            "Epoch 561/1000\n",
            "491/491 [==============================] - 0s 52us/step - loss: 0.4858 - acc: 0.7495 - val_loss: 0.5443 - val_acc: 0.6829\n",
            "Epoch 562/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4867 - acc: 0.7475 - val_loss: 0.5442 - val_acc: 0.6911\n",
            "Epoch 563/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4854 - acc: 0.7475 - val_loss: 0.5441 - val_acc: 0.6992\n",
            "Epoch 564/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4853 - acc: 0.7495 - val_loss: 0.5441 - val_acc: 0.6992\n",
            "Epoch 565/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4853 - acc: 0.7475 - val_loss: 0.5440 - val_acc: 0.6992\n",
            "Epoch 566/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4848 - acc: 0.7495 - val_loss: 0.5439 - val_acc: 0.6911\n",
            "Epoch 567/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4846 - acc: 0.7475 - val_loss: 0.5438 - val_acc: 0.6911\n",
            "Epoch 568/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4845 - acc: 0.7515 - val_loss: 0.5438 - val_acc: 0.6911\n",
            "Epoch 569/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4842 - acc: 0.7536 - val_loss: 0.5440 - val_acc: 0.6911\n",
            "Epoch 570/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4838 - acc: 0.7495 - val_loss: 0.5436 - val_acc: 0.6911\n",
            "Epoch 571/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4838 - acc: 0.7475 - val_loss: 0.5435 - val_acc: 0.6911\n",
            "Epoch 572/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4836 - acc: 0.7515 - val_loss: 0.5435 - val_acc: 0.6829\n",
            "Epoch 573/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4837 - acc: 0.7536 - val_loss: 0.5434 - val_acc: 0.6911\n",
            "Epoch 574/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4834 - acc: 0.7515 - val_loss: 0.5436 - val_acc: 0.6911\n",
            "Epoch 575/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4830 - acc: 0.7454 - val_loss: 0.5434 - val_acc: 0.6911\n",
            "Epoch 576/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4827 - acc: 0.7495 - val_loss: 0.5433 - val_acc: 0.6911\n",
            "Epoch 577/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4826 - acc: 0.7495 - val_loss: 0.5433 - val_acc: 0.6911\n",
            "Epoch 578/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4823 - acc: 0.7495 - val_loss: 0.5433 - val_acc: 0.6911\n",
            "Epoch 579/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4829 - acc: 0.7556 - val_loss: 0.5435 - val_acc: 0.6911\n",
            "Epoch 580/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4824 - acc: 0.7454 - val_loss: 0.5431 - val_acc: 0.6911\n",
            "Epoch 581/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4814 - acc: 0.7495 - val_loss: 0.5432 - val_acc: 0.6911\n",
            "Epoch 582/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4814 - acc: 0.7495 - val_loss: 0.5431 - val_acc: 0.6911\n",
            "Epoch 583/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4810 - acc: 0.7475 - val_loss: 0.5429 - val_acc: 0.6911\n",
            "Epoch 584/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4812 - acc: 0.7475 - val_loss: 0.5428 - val_acc: 0.6911\n",
            "Epoch 585/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4810 - acc: 0.7495 - val_loss: 0.5427 - val_acc: 0.6911\n",
            "Epoch 586/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4803 - acc: 0.7515 - val_loss: 0.5427 - val_acc: 0.6992\n",
            "Epoch 587/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4806 - acc: 0.7495 - val_loss: 0.5426 - val_acc: 0.6992\n",
            "Epoch 588/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4799 - acc: 0.7475 - val_loss: 0.5425 - val_acc: 0.6911\n",
            "Epoch 589/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4802 - acc: 0.7515 - val_loss: 0.5425 - val_acc: 0.6992\n",
            "Epoch 590/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4798 - acc: 0.7495 - val_loss: 0.5425 - val_acc: 0.7073\n",
            "Epoch 591/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4796 - acc: 0.7515 - val_loss: 0.5425 - val_acc: 0.6911\n",
            "Epoch 592/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4792 - acc: 0.7495 - val_loss: 0.5423 - val_acc: 0.6992\n",
            "Epoch 593/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4789 - acc: 0.7495 - val_loss: 0.5424 - val_acc: 0.6992\n",
            "Epoch 594/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4786 - acc: 0.7536 - val_loss: 0.5424 - val_acc: 0.6992\n",
            "Epoch 595/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4785 - acc: 0.7515 - val_loss: 0.5424 - val_acc: 0.6992\n",
            "Epoch 596/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4783 - acc: 0.7475 - val_loss: 0.5423 - val_acc: 0.7073\n",
            "Epoch 597/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4783 - acc: 0.7536 - val_loss: 0.5422 - val_acc: 0.6992\n",
            "Epoch 598/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4781 - acc: 0.7556 - val_loss: 0.5420 - val_acc: 0.6992\n",
            "Epoch 599/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4777 - acc: 0.7515 - val_loss: 0.5420 - val_acc: 0.6992\n",
            "Epoch 600/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4778 - acc: 0.7515 - val_loss: 0.5419 - val_acc: 0.6992\n",
            "Epoch 601/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4773 - acc: 0.7536 - val_loss: 0.5419 - val_acc: 0.6992\n",
            "Epoch 602/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4770 - acc: 0.7536 - val_loss: 0.5419 - val_acc: 0.6992\n",
            "Epoch 603/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4766 - acc: 0.7536 - val_loss: 0.5418 - val_acc: 0.6992\n",
            "Epoch 604/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4768 - acc: 0.7536 - val_loss: 0.5419 - val_acc: 0.6911\n",
            "Epoch 605/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4774 - acc: 0.7475 - val_loss: 0.5417 - val_acc: 0.6992\n",
            "Epoch 606/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4767 - acc: 0.7515 - val_loss: 0.5417 - val_acc: 0.6992\n",
            "Epoch 607/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4765 - acc: 0.7556 - val_loss: 0.5417 - val_acc: 0.6992\n",
            "Epoch 608/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4761 - acc: 0.7576 - val_loss: 0.5417 - val_acc: 0.7073\n",
            "Epoch 609/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4762 - acc: 0.7556 - val_loss: 0.5417 - val_acc: 0.6992\n",
            "Epoch 610/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4754 - acc: 0.7576 - val_loss: 0.5415 - val_acc: 0.6992\n",
            "Epoch 611/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4756 - acc: 0.7536 - val_loss: 0.5415 - val_acc: 0.6992\n",
            "Epoch 612/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4753 - acc: 0.7515 - val_loss: 0.5415 - val_acc: 0.6992\n",
            "Epoch 613/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4749 - acc: 0.7576 - val_loss: 0.5414 - val_acc: 0.6992\n",
            "Epoch 614/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4748 - acc: 0.7617 - val_loss: 0.5415 - val_acc: 0.6992\n",
            "Epoch 615/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4754 - acc: 0.7576 - val_loss: 0.5414 - val_acc: 0.6992\n",
            "Epoch 616/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4751 - acc: 0.7576 - val_loss: 0.5413 - val_acc: 0.6992\n",
            "Epoch 617/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4749 - acc: 0.7617 - val_loss: 0.5413 - val_acc: 0.6992\n",
            "Epoch 618/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4749 - acc: 0.7597 - val_loss: 0.5413 - val_acc: 0.6992\n",
            "Epoch 619/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4738 - acc: 0.7597 - val_loss: 0.5413 - val_acc: 0.6992\n",
            "Epoch 620/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4739 - acc: 0.7536 - val_loss: 0.5413 - val_acc: 0.6992\n",
            "Epoch 621/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4743 - acc: 0.7556 - val_loss: 0.5413 - val_acc: 0.6992\n",
            "Epoch 622/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4738 - acc: 0.7576 - val_loss: 0.5412 - val_acc: 0.6992\n",
            "Epoch 623/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4736 - acc: 0.7536 - val_loss: 0.5412 - val_acc: 0.6992\n",
            "Epoch 624/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4729 - acc: 0.7597 - val_loss: 0.5412 - val_acc: 0.6992\n",
            "Epoch 625/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4732 - acc: 0.7617 - val_loss: 0.5411 - val_acc: 0.6992\n",
            "Epoch 626/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4730 - acc: 0.7576 - val_loss: 0.5411 - val_acc: 0.6992\n",
            "Epoch 627/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4733 - acc: 0.7597 - val_loss: 0.5411 - val_acc: 0.6992\n",
            "Epoch 628/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4725 - acc: 0.7576 - val_loss: 0.5411 - val_acc: 0.6992\n",
            "Epoch 629/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4725 - acc: 0.7576 - val_loss: 0.5410 - val_acc: 0.6992\n",
            "Epoch 630/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4722 - acc: 0.7617 - val_loss: 0.5410 - val_acc: 0.6992\n",
            "Epoch 631/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4721 - acc: 0.7658 - val_loss: 0.5410 - val_acc: 0.6992\n",
            "Epoch 632/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4720 - acc: 0.7637 - val_loss: 0.5411 - val_acc: 0.7154\n",
            "Epoch 633/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4722 - acc: 0.7658 - val_loss: 0.5410 - val_acc: 0.6992\n",
            "Epoch 634/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4720 - acc: 0.7597 - val_loss: 0.5410 - val_acc: 0.6992\n",
            "Epoch 635/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4711 - acc: 0.7617 - val_loss: 0.5410 - val_acc: 0.6992\n",
            "Epoch 636/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4712 - acc: 0.7617 - val_loss: 0.5409 - val_acc: 0.6992\n",
            "Epoch 637/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4716 - acc: 0.7637 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 638/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4711 - acc: 0.7597 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 639/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4706 - acc: 0.7637 - val_loss: 0.5409 - val_acc: 0.6992\n",
            "Epoch 640/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4710 - acc: 0.7597 - val_loss: 0.5410 - val_acc: 0.6992\n",
            "Epoch 641/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4702 - acc: 0.7678 - val_loss: 0.5409 - val_acc: 0.6992\n",
            "Epoch 642/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4705 - acc: 0.7637 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 643/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4701 - acc: 0.7617 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 644/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4700 - acc: 0.7617 - val_loss: 0.5408 - val_acc: 0.6992\n",
            "Epoch 645/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4700 - acc: 0.7637 - val_loss: 0.5408 - val_acc: 0.6992\n",
            "Epoch 646/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4696 - acc: 0.7617 - val_loss: 0.5408 - val_acc: 0.6992\n",
            "Epoch 647/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4696 - acc: 0.7637 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 648/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4695 - acc: 0.7617 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 649/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4694 - acc: 0.7658 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 650/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4690 - acc: 0.7637 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 651/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4690 - acc: 0.7678 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 652/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4687 - acc: 0.7658 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 653/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4694 - acc: 0.7658 - val_loss: 0.5408 - val_acc: 0.6992\n",
            "Epoch 654/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4688 - acc: 0.7658 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 655/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4688 - acc: 0.7678 - val_loss: 0.5409 - val_acc: 0.6992\n",
            "Epoch 656/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4688 - acc: 0.7637 - val_loss: 0.5409 - val_acc: 0.6992\n",
            "Epoch 657/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4682 - acc: 0.7637 - val_loss: 0.5409 - val_acc: 0.6992\n",
            "Epoch 658/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4680 - acc: 0.7678 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 659/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4678 - acc: 0.7658 - val_loss: 0.5408 - val_acc: 0.6992\n",
            "Epoch 660/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4676 - acc: 0.7658 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 661/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4676 - acc: 0.7658 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 662/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4672 - acc: 0.7658 - val_loss: 0.5406 - val_acc: 0.7073\n",
            "Epoch 663/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4674 - acc: 0.7678 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 664/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4680 - acc: 0.7719 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 665/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4671 - acc: 0.7699 - val_loss: 0.5406 - val_acc: 0.7154\n",
            "Epoch 666/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4667 - acc: 0.7699 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 667/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4666 - acc: 0.7699 - val_loss: 0.5406 - val_acc: 0.7073\n",
            "Epoch 668/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4666 - acc: 0.7760 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 669/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4663 - acc: 0.7719 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 670/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4662 - acc: 0.7699 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 671/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4663 - acc: 0.7699 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 672/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4658 - acc: 0.7719 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 673/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4668 - acc: 0.7678 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 674/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4656 - acc: 0.7678 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 675/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4660 - acc: 0.7699 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 676/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4653 - acc: 0.7699 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 677/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4662 - acc: 0.7699 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 678/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4653 - acc: 0.7739 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 679/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4650 - acc: 0.7699 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 680/1000\n",
            "491/491 [==============================] - 0s 44us/step - loss: 0.4655 - acc: 0.7678 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 681/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4649 - acc: 0.7719 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 682/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4648 - acc: 0.7699 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 683/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4649 - acc: 0.7678 - val_loss: 0.5408 - val_acc: 0.7154\n",
            "Epoch 684/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4649 - acc: 0.7719 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 685/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4644 - acc: 0.7739 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 686/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4647 - acc: 0.7760 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 687/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4643 - acc: 0.7739 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 688/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4638 - acc: 0.7760 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 689/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4640 - acc: 0.7739 - val_loss: 0.5413 - val_acc: 0.7154\n",
            "Epoch 690/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4644 - acc: 0.7658 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 691/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4640 - acc: 0.7719 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 692/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4636 - acc: 0.7780 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 693/1000\n",
            "491/491 [==============================] - 0s 49us/step - loss: 0.4635 - acc: 0.7739 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 694/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4634 - acc: 0.7699 - val_loss: 0.5407 - val_acc: 0.7073\n",
            "Epoch 695/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4634 - acc: 0.7739 - val_loss: 0.5408 - val_acc: 0.7073\n",
            "Epoch 696/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4631 - acc: 0.7760 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 697/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4629 - acc: 0.7760 - val_loss: 0.5413 - val_acc: 0.7154\n",
            "Epoch 698/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4632 - acc: 0.7780 - val_loss: 0.5412 - val_acc: 0.7073\n",
            "Epoch 699/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4624 - acc: 0.7719 - val_loss: 0.5409 - val_acc: 0.7073\n",
            "Epoch 700/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4627 - acc: 0.7760 - val_loss: 0.5411 - val_acc: 0.7073\n",
            "Epoch 701/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4629 - acc: 0.7739 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 702/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4619 - acc: 0.7760 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 703/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4621 - acc: 0.7760 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 704/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4618 - acc: 0.7760 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 705/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4619 - acc: 0.7760 - val_loss: 0.5409 - val_acc: 0.7154\n",
            "Epoch 706/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4619 - acc: 0.7821 - val_loss: 0.5409 - val_acc: 0.7154\n",
            "Epoch 707/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4631 - acc: 0.7678 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 708/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4613 - acc: 0.7760 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 709/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4616 - acc: 0.7780 - val_loss: 0.5411 - val_acc: 0.7073\n",
            "Epoch 710/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4610 - acc: 0.7760 - val_loss: 0.5410 - val_acc: 0.7073\n",
            "Epoch 711/1000\n",
            "491/491 [==============================] - 0s 54us/step - loss: 0.4613 - acc: 0.7841 - val_loss: 0.5412 - val_acc: 0.7073\n",
            "Epoch 712/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4610 - acc: 0.7780 - val_loss: 0.5412 - val_acc: 0.7073\n",
            "Epoch 713/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4610 - acc: 0.7800 - val_loss: 0.5412 - val_acc: 0.7073\n",
            "Epoch 714/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4610 - acc: 0.7780 - val_loss: 0.5411 - val_acc: 0.7073\n",
            "Epoch 715/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4612 - acc: 0.7800 - val_loss: 0.5414 - val_acc: 0.7154\n",
            "Epoch 716/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4607 - acc: 0.7800 - val_loss: 0.5416 - val_acc: 0.7154\n",
            "Epoch 717/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4606 - acc: 0.7800 - val_loss: 0.5413 - val_acc: 0.7154\n",
            "Epoch 718/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4607 - acc: 0.7739 - val_loss: 0.5412 - val_acc: 0.7073\n",
            "Epoch 719/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4604 - acc: 0.7821 - val_loss: 0.5412 - val_acc: 0.7073\n",
            "Epoch 720/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4606 - acc: 0.7821 - val_loss: 0.5415 - val_acc: 0.7154\n",
            "Epoch 721/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4618 - acc: 0.7800 - val_loss: 0.5418 - val_acc: 0.7154\n",
            "Epoch 722/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4602 - acc: 0.7800 - val_loss: 0.5413 - val_acc: 0.7073\n",
            "Epoch 723/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4599 - acc: 0.7800 - val_loss: 0.5412 - val_acc: 0.7154\n",
            "Epoch 724/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4596 - acc: 0.7760 - val_loss: 0.5419 - val_acc: 0.7154\n",
            "Epoch 725/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4600 - acc: 0.7841 - val_loss: 0.5413 - val_acc: 0.7073\n",
            "Epoch 726/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4595 - acc: 0.7821 - val_loss: 0.5415 - val_acc: 0.7154\n",
            "Epoch 727/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4600 - acc: 0.7800 - val_loss: 0.5418 - val_acc: 0.7154\n",
            "Epoch 728/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4597 - acc: 0.7800 - val_loss: 0.5412 - val_acc: 0.7154\n",
            "Epoch 729/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4592 - acc: 0.7821 - val_loss: 0.5415 - val_acc: 0.7154\n",
            "Epoch 730/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4594 - acc: 0.7780 - val_loss: 0.5415 - val_acc: 0.7154\n",
            "Epoch 731/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4592 - acc: 0.7699 - val_loss: 0.5414 - val_acc: 0.7073\n",
            "Epoch 732/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4591 - acc: 0.7821 - val_loss: 0.5415 - val_acc: 0.7073\n",
            "Epoch 733/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4589 - acc: 0.7841 - val_loss: 0.5415 - val_acc: 0.7073\n",
            "Epoch 734/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4588 - acc: 0.7780 - val_loss: 0.5416 - val_acc: 0.7073\n",
            "Epoch 735/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4587 - acc: 0.7800 - val_loss: 0.5414 - val_acc: 0.7073\n",
            "Epoch 736/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4588 - acc: 0.7841 - val_loss: 0.5418 - val_acc: 0.7073\n",
            "Epoch 737/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4584 - acc: 0.7841 - val_loss: 0.5418 - val_acc: 0.7073\n",
            "Epoch 738/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4582 - acc: 0.7841 - val_loss: 0.5417 - val_acc: 0.7073\n",
            "Epoch 739/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4587 - acc: 0.7821 - val_loss: 0.5424 - val_acc: 0.7154\n",
            "Epoch 740/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4587 - acc: 0.7841 - val_loss: 0.5418 - val_acc: 0.7073\n",
            "Epoch 741/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4581 - acc: 0.7821 - val_loss: 0.5418 - val_acc: 0.7073\n",
            "Epoch 742/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4587 - acc: 0.7821 - val_loss: 0.5417 - val_acc: 0.7073\n",
            "Epoch 743/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4581 - acc: 0.7800 - val_loss: 0.5418 - val_acc: 0.7073\n",
            "Epoch 744/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4578 - acc: 0.7800 - val_loss: 0.5419 - val_acc: 0.7154\n",
            "Epoch 745/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4574 - acc: 0.7800 - val_loss: 0.5420 - val_acc: 0.7154\n",
            "Epoch 746/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4576 - acc: 0.7862 - val_loss: 0.5423 - val_acc: 0.7154\n",
            "Epoch 747/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4571 - acc: 0.7800 - val_loss: 0.5428 - val_acc: 0.7154\n",
            "Epoch 748/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4579 - acc: 0.7800 - val_loss: 0.5428 - val_acc: 0.7154\n",
            "Epoch 749/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4575 - acc: 0.7821 - val_loss: 0.5428 - val_acc: 0.7154\n",
            "Epoch 750/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4578 - acc: 0.7780 - val_loss: 0.5423 - val_acc: 0.7154\n",
            "Epoch 751/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4576 - acc: 0.7800 - val_loss: 0.5425 - val_acc: 0.7154\n",
            "Epoch 752/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4569 - acc: 0.7800 - val_loss: 0.5417 - val_acc: 0.7073\n",
            "Epoch 753/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4569 - acc: 0.7821 - val_loss: 0.5422 - val_acc: 0.7154\n",
            "Epoch 754/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4565 - acc: 0.7821 - val_loss: 0.5424 - val_acc: 0.7154\n",
            "Epoch 755/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4573 - acc: 0.7841 - val_loss: 0.5426 - val_acc: 0.7154\n",
            "Epoch 756/1000\n",
            "491/491 [==============================] - 0s 29us/step - loss: 0.4567 - acc: 0.7841 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 757/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4566 - acc: 0.7780 - val_loss: 0.5426 - val_acc: 0.7154\n",
            "Epoch 758/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4563 - acc: 0.7862 - val_loss: 0.5419 - val_acc: 0.7073\n",
            "Epoch 759/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4568 - acc: 0.7841 - val_loss: 0.5420 - val_acc: 0.7073\n",
            "Epoch 760/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4566 - acc: 0.7800 - val_loss: 0.5423 - val_acc: 0.7073\n",
            "Epoch 761/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4563 - acc: 0.7800 - val_loss: 0.5427 - val_acc: 0.7154\n",
            "Epoch 762/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4560 - acc: 0.7841 - val_loss: 0.5424 - val_acc: 0.7073\n",
            "Epoch 763/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4568 - acc: 0.7862 - val_loss: 0.5427 - val_acc: 0.7154\n",
            "Epoch 764/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4556 - acc: 0.7862 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 765/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4560 - acc: 0.7841 - val_loss: 0.5431 - val_acc: 0.7154\n",
            "Epoch 766/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4557 - acc: 0.7862 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 767/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4556 - acc: 0.7862 - val_loss: 0.5422 - val_acc: 0.7073\n",
            "Epoch 768/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4559 - acc: 0.7821 - val_loss: 0.5423 - val_acc: 0.7073\n",
            "Epoch 769/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4554 - acc: 0.7760 - val_loss: 0.5427 - val_acc: 0.7073\n",
            "Epoch 770/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4560 - acc: 0.7862 - val_loss: 0.5428 - val_acc: 0.7154\n",
            "Epoch 771/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4554 - acc: 0.7841 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 772/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4551 - acc: 0.7862 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 773/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4550 - acc: 0.7902 - val_loss: 0.5439 - val_acc: 0.7236\n",
            "Epoch 774/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4554 - acc: 0.7841 - val_loss: 0.5428 - val_acc: 0.7073\n",
            "Epoch 775/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4548 - acc: 0.7841 - val_loss: 0.5437 - val_acc: 0.7236\n",
            "Epoch 776/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4554 - acc: 0.7841 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 777/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4543 - acc: 0.7841 - val_loss: 0.5428 - val_acc: 0.7073\n",
            "Epoch 778/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4546 - acc: 0.7841 - val_loss: 0.5429 - val_acc: 0.7073\n",
            "Epoch 779/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4546 - acc: 0.7821 - val_loss: 0.5431 - val_acc: 0.7236\n",
            "Epoch 780/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4544 - acc: 0.7862 - val_loss: 0.5447 - val_acc: 0.7236\n",
            "Epoch 781/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4546 - acc: 0.7882 - val_loss: 0.5434 - val_acc: 0.7236\n",
            "Epoch 782/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4543 - acc: 0.7841 - val_loss: 0.5436 - val_acc: 0.7236\n",
            "Epoch 783/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4543 - acc: 0.7841 - val_loss: 0.5429 - val_acc: 0.7073\n",
            "Epoch 784/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4538 - acc: 0.7862 - val_loss: 0.5432 - val_acc: 0.7236\n",
            "Epoch 785/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4542 - acc: 0.7862 - val_loss: 0.5442 - val_acc: 0.7236\n",
            "Epoch 786/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4540 - acc: 0.7841 - val_loss: 0.5433 - val_acc: 0.7236\n",
            "Epoch 787/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4542 - acc: 0.7862 - val_loss: 0.5440 - val_acc: 0.7236\n",
            "Epoch 788/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4542 - acc: 0.7841 - val_loss: 0.5445 - val_acc: 0.7236\n",
            "Epoch 789/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4540 - acc: 0.7821 - val_loss: 0.5426 - val_acc: 0.7073\n",
            "Epoch 790/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4537 - acc: 0.7841 - val_loss: 0.5424 - val_acc: 0.6992\n",
            "Epoch 791/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4540 - acc: 0.7841 - val_loss: 0.5425 - val_acc: 0.6992\n",
            "Epoch 792/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4536 - acc: 0.7841 - val_loss: 0.5424 - val_acc: 0.6992\n",
            "Epoch 793/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4536 - acc: 0.7862 - val_loss: 0.5426 - val_acc: 0.6992\n",
            "Epoch 794/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4533 - acc: 0.7841 - val_loss: 0.5425 - val_acc: 0.6992\n",
            "Epoch 795/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4545 - acc: 0.7882 - val_loss: 0.5427 - val_acc: 0.7073\n",
            "Epoch 796/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4549 - acc: 0.7841 - val_loss: 0.5429 - val_acc: 0.7154\n",
            "Epoch 797/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4538 - acc: 0.7800 - val_loss: 0.5448 - val_acc: 0.7236\n",
            "Epoch 798/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4534 - acc: 0.7821 - val_loss: 0.5436 - val_acc: 0.7236\n",
            "Epoch 799/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4530 - acc: 0.7841 - val_loss: 0.5437 - val_acc: 0.7236\n",
            "Epoch 800/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4528 - acc: 0.7841 - val_loss: 0.5431 - val_acc: 0.7154\n",
            "Epoch 801/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4527 - acc: 0.7841 - val_loss: 0.5435 - val_acc: 0.7236\n",
            "Epoch 802/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4527 - acc: 0.7862 - val_loss: 0.5438 - val_acc: 0.7236\n",
            "Epoch 803/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4524 - acc: 0.7882 - val_loss: 0.5431 - val_acc: 0.7154\n",
            "Epoch 804/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4527 - acc: 0.7821 - val_loss: 0.5426 - val_acc: 0.6992\n",
            "Epoch 805/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4531 - acc: 0.7902 - val_loss: 0.5433 - val_acc: 0.7154\n",
            "Epoch 806/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4531 - acc: 0.7841 - val_loss: 0.5436 - val_acc: 0.7236\n",
            "Epoch 807/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4532 - acc: 0.7882 - val_loss: 0.5437 - val_acc: 0.7236\n",
            "Epoch 808/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4532 - acc: 0.7862 - val_loss: 0.5431 - val_acc: 0.7154\n",
            "Epoch 809/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4530 - acc: 0.7841 - val_loss: 0.5433 - val_acc: 0.7154\n",
            "Epoch 810/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4521 - acc: 0.7862 - val_loss: 0.5436 - val_acc: 0.7154\n",
            "Epoch 811/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4528 - acc: 0.7841 - val_loss: 0.5436 - val_acc: 0.7236\n",
            "Epoch 812/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4519 - acc: 0.7841 - val_loss: 0.5435 - val_acc: 0.7154\n",
            "Epoch 813/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4522 - acc: 0.7862 - val_loss: 0.5438 - val_acc: 0.7236\n",
            "Epoch 814/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4519 - acc: 0.7821 - val_loss: 0.5432 - val_acc: 0.7154\n",
            "Epoch 815/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4522 - acc: 0.7862 - val_loss: 0.5434 - val_acc: 0.7154\n",
            "Epoch 816/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4523 - acc: 0.7862 - val_loss: 0.5448 - val_acc: 0.7236\n",
            "Epoch 817/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4526 - acc: 0.7821 - val_loss: 0.5438 - val_acc: 0.7154\n",
            "Epoch 818/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4514 - acc: 0.7862 - val_loss: 0.5441 - val_acc: 0.7154\n",
            "Epoch 819/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4518 - acc: 0.7841 - val_loss: 0.5440 - val_acc: 0.7154\n",
            "Epoch 820/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4513 - acc: 0.7862 - val_loss: 0.5436 - val_acc: 0.7154\n",
            "Epoch 821/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4516 - acc: 0.7841 - val_loss: 0.5434 - val_acc: 0.7154\n",
            "Epoch 822/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4512 - acc: 0.7841 - val_loss: 0.5438 - val_acc: 0.7154\n",
            "Epoch 823/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4510 - acc: 0.7923 - val_loss: 0.5438 - val_acc: 0.7154\n",
            "Epoch 824/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4511 - acc: 0.7862 - val_loss: 0.5436 - val_acc: 0.7154\n",
            "Epoch 825/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4512 - acc: 0.7882 - val_loss: 0.5436 - val_acc: 0.7154\n",
            "Epoch 826/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4515 - acc: 0.7841 - val_loss: 0.5442 - val_acc: 0.7154\n",
            "Epoch 827/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4515 - acc: 0.7862 - val_loss: 0.5442 - val_acc: 0.7154\n",
            "Epoch 828/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4510 - acc: 0.7862 - val_loss: 0.5438 - val_acc: 0.7154\n",
            "Epoch 829/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4511 - acc: 0.7902 - val_loss: 0.5448 - val_acc: 0.7236\n",
            "Epoch 830/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4518 - acc: 0.7902 - val_loss: 0.5447 - val_acc: 0.7236\n",
            "Epoch 831/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4507 - acc: 0.7882 - val_loss: 0.5447 - val_acc: 0.7236\n",
            "Epoch 832/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4510 - acc: 0.7821 - val_loss: 0.5447 - val_acc: 0.7154\n",
            "Epoch 833/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4508 - acc: 0.7841 - val_loss: 0.5443 - val_acc: 0.7154\n",
            "Epoch 834/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4507 - acc: 0.7862 - val_loss: 0.5441 - val_acc: 0.7154\n",
            "Epoch 835/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4507 - acc: 0.7882 - val_loss: 0.5447 - val_acc: 0.7154\n",
            "Epoch 836/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4502 - acc: 0.7841 - val_loss: 0.5442 - val_acc: 0.7154\n",
            "Epoch 837/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4512 - acc: 0.7923 - val_loss: 0.5448 - val_acc: 0.7154\n",
            "Epoch 838/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4504 - acc: 0.7862 - val_loss: 0.5440 - val_acc: 0.7154\n",
            "Epoch 839/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4507 - acc: 0.7882 - val_loss: 0.5449 - val_acc: 0.7154\n",
            "Epoch 840/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4507 - acc: 0.7821 - val_loss: 0.5440 - val_acc: 0.7154\n",
            "Epoch 841/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4502 - acc: 0.7841 - val_loss: 0.5435 - val_acc: 0.7073\n",
            "Epoch 842/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4513 - acc: 0.7902 - val_loss: 0.5437 - val_acc: 0.7154\n",
            "Epoch 843/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4510 - acc: 0.7902 - val_loss: 0.5439 - val_acc: 0.7154\n",
            "Epoch 844/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4499 - acc: 0.7923 - val_loss: 0.5440 - val_acc: 0.7154\n",
            "Epoch 845/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4510 - acc: 0.7862 - val_loss: 0.5452 - val_acc: 0.7236\n",
            "Epoch 846/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4500 - acc: 0.7862 - val_loss: 0.5444 - val_acc: 0.7154\n",
            "Epoch 847/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4508 - acc: 0.7821 - val_loss: 0.5438 - val_acc: 0.7154\n",
            "Epoch 848/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4502 - acc: 0.7902 - val_loss: 0.5435 - val_acc: 0.7073\n",
            "Epoch 849/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4502 - acc: 0.7963 - val_loss: 0.5441 - val_acc: 0.7154\n",
            "Epoch 850/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4501 - acc: 0.7902 - val_loss: 0.5447 - val_acc: 0.7154\n",
            "Epoch 851/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4502 - acc: 0.7882 - val_loss: 0.5439 - val_acc: 0.7154\n",
            "Epoch 852/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4493 - acc: 0.7882 - val_loss: 0.5439 - val_acc: 0.7154\n",
            "Epoch 853/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4499 - acc: 0.7902 - val_loss: 0.5444 - val_acc: 0.7154\n",
            "Epoch 854/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4492 - acc: 0.7862 - val_loss: 0.5443 - val_acc: 0.7154\n",
            "Epoch 855/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4492 - acc: 0.7923 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 856/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4495 - acc: 0.7821 - val_loss: 0.5444 - val_acc: 0.7154\n",
            "Epoch 857/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4491 - acc: 0.7882 - val_loss: 0.5448 - val_acc: 0.7154\n",
            "Epoch 858/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4496 - acc: 0.7882 - val_loss: 0.5444 - val_acc: 0.7154\n",
            "Epoch 859/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4491 - acc: 0.7943 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 860/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4501 - acc: 0.7862 - val_loss: 0.5446 - val_acc: 0.7154\n",
            "Epoch 861/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4496 - acc: 0.7902 - val_loss: 0.5453 - val_acc: 0.7154\n",
            "Epoch 862/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4496 - acc: 0.7862 - val_loss: 0.5466 - val_acc: 0.7236\n",
            "Epoch 863/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4492 - acc: 0.7862 - val_loss: 0.5457 - val_acc: 0.7154\n",
            "Epoch 864/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4491 - acc: 0.7862 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 865/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4498 - acc: 0.7882 - val_loss: 0.5444 - val_acc: 0.7154\n",
            "Epoch 866/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4495 - acc: 0.7882 - val_loss: 0.5443 - val_acc: 0.7154\n",
            "Epoch 867/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4487 - acc: 0.7963 - val_loss: 0.5447 - val_acc: 0.7154\n",
            "Epoch 868/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4501 - acc: 0.7902 - val_loss: 0.5447 - val_acc: 0.7154\n",
            "Epoch 869/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4483 - acc: 0.7923 - val_loss: 0.5451 - val_acc: 0.7154\n",
            "Epoch 870/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4490 - acc: 0.7943 - val_loss: 0.5454 - val_acc: 0.7154\n",
            "Epoch 871/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4481 - acc: 0.7902 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 872/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4481 - acc: 0.7923 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 873/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4481 - acc: 0.7882 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 874/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4482 - acc: 0.7862 - val_loss: 0.5444 - val_acc: 0.7154\n",
            "Epoch 875/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4480 - acc: 0.7943 - val_loss: 0.5446 - val_acc: 0.7154\n",
            "Epoch 876/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4488 - acc: 0.7902 - val_loss: 0.5446 - val_acc: 0.7154\n",
            "Epoch 877/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4491 - acc: 0.7902 - val_loss: 0.5449 - val_acc: 0.7154\n",
            "Epoch 878/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4483 - acc: 0.7882 - val_loss: 0.5461 - val_acc: 0.7154\n",
            "Epoch 879/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4482 - acc: 0.7902 - val_loss: 0.5454 - val_acc: 0.7154\n",
            "Epoch 880/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4485 - acc: 0.7882 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 881/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4478 - acc: 0.7923 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 882/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4484 - acc: 0.7902 - val_loss: 0.5454 - val_acc: 0.7154\n",
            "Epoch 883/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4483 - acc: 0.7923 - val_loss: 0.5460 - val_acc: 0.7154\n",
            "Epoch 884/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4486 - acc: 0.7902 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 885/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4478 - acc: 0.7882 - val_loss: 0.5449 - val_acc: 0.7154\n",
            "Epoch 886/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4482 - acc: 0.7902 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 887/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4485 - acc: 0.7943 - val_loss: 0.5461 - val_acc: 0.7154\n",
            "Epoch 888/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4482 - acc: 0.7923 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 889/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4476 - acc: 0.7882 - val_loss: 0.5459 - val_acc: 0.7154\n",
            "Epoch 890/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4476 - acc: 0.7943 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 891/1000\n",
            "491/491 [==============================] - 0s 42us/step - loss: 0.4474 - acc: 0.7923 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 892/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4471 - acc: 0.7923 - val_loss: 0.5449 - val_acc: 0.7154\n",
            "Epoch 893/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4474 - acc: 0.7963 - val_loss: 0.5447 - val_acc: 0.7154\n",
            "Epoch 894/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4484 - acc: 0.7882 - val_loss: 0.5457 - val_acc: 0.7154\n",
            "Epoch 895/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4475 - acc: 0.7882 - val_loss: 0.5466 - val_acc: 0.7154\n",
            "Epoch 896/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4482 - acc: 0.7841 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 897/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4474 - acc: 0.7882 - val_loss: 0.5459 - val_acc: 0.7154\n",
            "Epoch 898/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4476 - acc: 0.7862 - val_loss: 0.5449 - val_acc: 0.7154\n",
            "Epoch 899/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4482 - acc: 0.8024 - val_loss: 0.5459 - val_acc: 0.7154\n",
            "Epoch 900/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4465 - acc: 0.7882 - val_loss: 0.5446 - val_acc: 0.7154\n",
            "Epoch 901/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4473 - acc: 0.7963 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 902/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4476 - acc: 0.7902 - val_loss: 0.5446 - val_acc: 0.7154\n",
            "Epoch 903/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4475 - acc: 0.7943 - val_loss: 0.5445 - val_acc: 0.7073\n",
            "Epoch 904/1000\n",
            "491/491 [==============================] - 0s 29us/step - loss: 0.4472 - acc: 0.7943 - val_loss: 0.5446 - val_acc: 0.7154\n",
            "Epoch 905/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4474 - acc: 0.7902 - val_loss: 0.5445 - val_acc: 0.7154\n",
            "Epoch 906/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4470 - acc: 0.7882 - val_loss: 0.5443 - val_acc: 0.7073\n",
            "Epoch 907/1000\n",
            "491/491 [==============================] - 0s 46us/step - loss: 0.4471 - acc: 0.7923 - val_loss: 0.5446 - val_acc: 0.7154\n",
            "Epoch 908/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4467 - acc: 0.7943 - val_loss: 0.5451 - val_acc: 0.7154\n",
            "Epoch 909/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4468 - acc: 0.7963 - val_loss: 0.5447 - val_acc: 0.7154\n",
            "Epoch 910/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4474 - acc: 0.7882 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 911/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4461 - acc: 0.7943 - val_loss: 0.5449 - val_acc: 0.7154\n",
            "Epoch 912/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4465 - acc: 0.7943 - val_loss: 0.5448 - val_acc: 0.7154\n",
            "Epoch 913/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4467 - acc: 0.7902 - val_loss: 0.5451 - val_acc: 0.7154\n",
            "Epoch 914/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4472 - acc: 0.7943 - val_loss: 0.5446 - val_acc: 0.7073\n",
            "Epoch 915/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4468 - acc: 0.7963 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 916/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4467 - acc: 0.7943 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 917/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4463 - acc: 0.7923 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 918/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4471 - acc: 0.7943 - val_loss: 0.5448 - val_acc: 0.7073\n",
            "Epoch 919/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4469 - acc: 0.7923 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 920/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4458 - acc: 0.7984 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 921/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4465 - acc: 0.7902 - val_loss: 0.5457 - val_acc: 0.7154\n",
            "Epoch 922/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4469 - acc: 0.7943 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 923/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4463 - acc: 0.7902 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 924/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4455 - acc: 0.7963 - val_loss: 0.5450 - val_acc: 0.7154\n",
            "Epoch 925/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4469 - acc: 0.7923 - val_loss: 0.5468 - val_acc: 0.7154\n",
            "Epoch 926/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4472 - acc: 0.7923 - val_loss: 0.5461 - val_acc: 0.7154\n",
            "Epoch 927/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4465 - acc: 0.7923 - val_loss: 0.5462 - val_acc: 0.7154\n",
            "Epoch 928/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4457 - acc: 0.7943 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 929/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4463 - acc: 0.7963 - val_loss: 0.5451 - val_acc: 0.7154\n",
            "Epoch 930/1000\n",
            "491/491 [==============================] - 0s 40us/step - loss: 0.4455 - acc: 0.7882 - val_loss: 0.5459 - val_acc: 0.7154\n",
            "Epoch 931/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4455 - acc: 0.7943 - val_loss: 0.5466 - val_acc: 0.7154\n",
            "Epoch 932/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4460 - acc: 0.7882 - val_loss: 0.5453 - val_acc: 0.7154\n",
            "Epoch 933/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4456 - acc: 0.7963 - val_loss: 0.5461 - val_acc: 0.7154\n",
            "Epoch 934/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4458 - acc: 0.7902 - val_loss: 0.5473 - val_acc: 0.7154\n",
            "Epoch 935/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4467 - acc: 0.7902 - val_loss: 0.5459 - val_acc: 0.7154\n",
            "Epoch 936/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4457 - acc: 0.7963 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 937/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4453 - acc: 0.7923 - val_loss: 0.5455 - val_acc: 0.7154\n",
            "Epoch 938/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4450 - acc: 0.7984 - val_loss: 0.5454 - val_acc: 0.7154\n",
            "Epoch 939/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4462 - acc: 0.7963 - val_loss: 0.5461 - val_acc: 0.7154\n",
            "Epoch 940/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4452 - acc: 0.7943 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 941/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4456 - acc: 0.7923 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 942/1000\n",
            "491/491 [==============================] - 0s 43us/step - loss: 0.4458 - acc: 0.7984 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 943/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4450 - acc: 0.7963 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 944/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4456 - acc: 0.7984 - val_loss: 0.5451 - val_acc: 0.7073\n",
            "Epoch 945/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4450 - acc: 0.7923 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 946/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4448 - acc: 0.7902 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 947/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4451 - acc: 0.7902 - val_loss: 0.5448 - val_acc: 0.7236\n",
            "Epoch 948/1000\n",
            "491/491 [==============================] - 0s 35us/step - loss: 0.4450 - acc: 0.7923 - val_loss: 0.5453 - val_acc: 0.7073\n",
            "Epoch 949/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4454 - acc: 0.7963 - val_loss: 0.5459 - val_acc: 0.7154\n",
            "Epoch 950/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4455 - acc: 0.7923 - val_loss: 0.5452 - val_acc: 0.7073\n",
            "Epoch 951/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4449 - acc: 0.7923 - val_loss: 0.5460 - val_acc: 0.7154\n",
            "Epoch 952/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4445 - acc: 0.7943 - val_loss: 0.5457 - val_acc: 0.7154\n",
            "Epoch 953/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4451 - acc: 0.7943 - val_loss: 0.5452 - val_acc: 0.7073\n",
            "Epoch 954/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4448 - acc: 0.7923 - val_loss: 0.5465 - val_acc: 0.7154\n",
            "Epoch 955/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4455 - acc: 0.7902 - val_loss: 0.5452 - val_acc: 0.7154\n",
            "Epoch 956/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4448 - acc: 0.7923 - val_loss: 0.5453 - val_acc: 0.7073\n",
            "Epoch 957/1000\n",
            "491/491 [==============================] - 0s 38us/step - loss: 0.4446 - acc: 0.7923 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 958/1000\n",
            "491/491 [==============================] - 0s 45us/step - loss: 0.4448 - acc: 0.7963 - val_loss: 0.5461 - val_acc: 0.7154\n",
            "Epoch 959/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4454 - acc: 0.7943 - val_loss: 0.5458 - val_acc: 0.7154\n",
            "Epoch 960/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4442 - acc: 0.7902 - val_loss: 0.5461 - val_acc: 0.7154\n",
            "Epoch 961/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4445 - acc: 0.7963 - val_loss: 0.5462 - val_acc: 0.7154\n",
            "Epoch 962/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4454 - acc: 0.7984 - val_loss: 0.5456 - val_acc: 0.7154\n",
            "Epoch 963/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4440 - acc: 0.7943 - val_loss: 0.5465 - val_acc: 0.7154\n",
            "Epoch 964/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4443 - acc: 0.7963 - val_loss: 0.5458 - val_acc: 0.7154\n",
            "Epoch 965/1000\n",
            "491/491 [==============================] - 0s 36us/step - loss: 0.4445 - acc: 0.7902 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 966/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4442 - acc: 0.8024 - val_loss: 0.5464 - val_acc: 0.7154\n",
            "Epoch 967/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4445 - acc: 0.7963 - val_loss: 0.5454 - val_acc: 0.7073\n",
            "Epoch 968/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4440 - acc: 0.7943 - val_loss: 0.5458 - val_acc: 0.7154\n",
            "Epoch 969/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4452 - acc: 0.7923 - val_loss: 0.5458 - val_acc: 0.7154\n",
            "Epoch 970/1000\n",
            "491/491 [==============================] - 0s 41us/step - loss: 0.4437 - acc: 0.7984 - val_loss: 0.5453 - val_acc: 0.7073\n",
            "Epoch 971/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4444 - acc: 0.7902 - val_loss: 0.5452 - val_acc: 0.7236\n",
            "Epoch 972/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4438 - acc: 0.7923 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 973/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4438 - acc: 0.7984 - val_loss: 0.5459 - val_acc: 0.7073\n",
            "Epoch 974/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4450 - acc: 0.7902 - val_loss: 0.5458 - val_acc: 0.7073\n",
            "Epoch 975/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4439 - acc: 0.7943 - val_loss: 0.5464 - val_acc: 0.7154\n",
            "Epoch 976/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4439 - acc: 0.7902 - val_loss: 0.5471 - val_acc: 0.7154\n",
            "Epoch 977/1000\n",
            "491/491 [==============================] - 0s 30us/step - loss: 0.4448 - acc: 0.7943 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 978/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4436 - acc: 0.7984 - val_loss: 0.5467 - val_acc: 0.7154\n",
            "Epoch 979/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4438 - acc: 0.7943 - val_loss: 0.5483 - val_acc: 0.7154\n",
            "Epoch 980/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4442 - acc: 0.7923 - val_loss: 0.5468 - val_acc: 0.7154\n",
            "Epoch 981/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4438 - acc: 0.7923 - val_loss: 0.5464 - val_acc: 0.7154\n",
            "Epoch 982/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4445 - acc: 0.7902 - val_loss: 0.5462 - val_acc: 0.7154\n",
            "Epoch 983/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4442 - acc: 0.7923 - val_loss: 0.5464 - val_acc: 0.7154\n",
            "Epoch 984/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4433 - acc: 0.7943 - val_loss: 0.5456 - val_acc: 0.7236\n",
            "Epoch 985/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4456 - acc: 0.7943 - val_loss: 0.5456 - val_acc: 0.7236\n",
            "Epoch 986/1000\n",
            "491/491 [==============================] - 0s 55us/step - loss: 0.4439 - acc: 0.7923 - val_loss: 0.5461 - val_acc: 0.7073\n",
            "Epoch 987/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4433 - acc: 0.7963 - val_loss: 0.5463 - val_acc: 0.7154\n",
            "Epoch 988/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4432 - acc: 0.7902 - val_loss: 0.5479 - val_acc: 0.7154\n",
            "Epoch 989/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4436 - acc: 0.7923 - val_loss: 0.5479 - val_acc: 0.7154\n",
            "Epoch 990/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4432 - acc: 0.7923 - val_loss: 0.5466 - val_acc: 0.7154\n",
            "Epoch 991/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4429 - acc: 0.7943 - val_loss: 0.5459 - val_acc: 0.7236\n",
            "Epoch 992/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4439 - acc: 0.7923 - val_loss: 0.5460 - val_acc: 0.7236\n",
            "Epoch 993/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4432 - acc: 0.7963 - val_loss: 0.5466 - val_acc: 0.7154\n",
            "Epoch 994/1000\n",
            "491/491 [==============================] - 0s 32us/step - loss: 0.4436 - acc: 0.7943 - val_loss: 0.5471 - val_acc: 0.7154\n",
            "Epoch 995/1000\n",
            "491/491 [==============================] - 0s 39us/step - loss: 0.4434 - acc: 0.8004 - val_loss: 0.5476 - val_acc: 0.7154\n",
            "Epoch 996/1000\n",
            "491/491 [==============================] - 0s 37us/step - loss: 0.4435 - acc: 0.7943 - val_loss: 0.5477 - val_acc: 0.7154\n",
            "Epoch 997/1000\n",
            "491/491 [==============================] - 0s 31us/step - loss: 0.4441 - acc: 0.7943 - val_loss: 0.5468 - val_acc: 0.7154\n",
            "Epoch 998/1000\n",
            "491/491 [==============================] - 0s 33us/step - loss: 0.4433 - acc: 0.7963 - val_loss: 0.5464 - val_acc: 0.7154\n",
            "Epoch 999/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4432 - acc: 0.7963 - val_loss: 0.5464 - val_acc: 0.7073\n",
            "Epoch 1000/1000\n",
            "491/491 [==============================] - 0s 34us/step - loss: 0.4430 - acc: 0.7984 - val_loss: 0.5465 - val_acc: 0.7073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6QmRch8uJp4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "637487a2-cff0-406a-a7f3-b84b1a804b73"
      },
      "source": [
        "#visualize the training loss and the validation loss to see if the model is overfitting\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAE0CAYAAAC8ZD1pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9RcsqdPjAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0gG6bsWudFk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "fe30c137-e538-4b55-efa1-f3c0038bcb4b"
      },
      "source": [
        "#visualize the training accuracy and the validation accuracy to see if the model is overfitting\n",
        "plt.plot(hist.history['acc'])\n",
        "plt.plot(hist.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb8AAAE0CAYAAAC8ZD1pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCArnAa6ufsC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "b099bb6f-e15a-4ba2-bd47-44c2e93e7e6e"
      },
      "source": [
        "#Make a prediction & print the actual values\n",
        "prediction = model.predict(X_test)\n",
        "prediction  = [1 if y>=0.5 else 0 for y in prediction] #Threshold\n",
        "print(prediction)\n",
        "print(y_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n",
            "[0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1.\n",
            " 1. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1.\n",
            " 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4-m9S2oui1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "f1316ef2-3273-406d-93f5-ece92e9f437b"
      },
      "source": [
        "#Evaluate the model on the training data set\n",
        "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
        "pred = model.predict(X_train)\n",
        "pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n",
        "print(classification_report(y_train ,pred ))\n",
        "print('Confusion Matrix: \\n',confusion_matrix(y_train,pred))\n",
        "print()\n",
        "print('Accuracy: ', accuracy_score(y_train,pred))\n",
        "print()\n",
        "\n",
        "#Print the predictions\n",
        "#print('Predicted value: ',model.predict(X_train))\n",
        "\n",
        "#Print Actual Label\n",
        "#print('Actual value: ',y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.86      0.83       398\n",
            "         1.0       0.71      0.62      0.66       216\n",
            "\n",
            "    accuracy                           0.78       614\n",
            "   macro avg       0.76      0.74      0.75       614\n",
            "weighted avg       0.77      0.78      0.77       614\n",
            "\n",
            "Confusion Matrix: \n",
            " [[344  54]\n",
            " [ 83 133]]\n",
            "\n",
            "Accuracy:  0.7768729641693811\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs4eQgr0ulft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "1ada56c1-c642-4fe5-ba86-acd0b2b51f8d"
      },
      "source": [
        "#Evaluate the model on the test data set\n",
        "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
        "pred = model.predict(X_test)\n",
        "pred  = [1 if y>=0.5 else 0 for y in pred] #Threshold\n",
        "print(classification_report(y_test ,pred ))\n",
        "print('Confusion Matrix: \\n',confusion_matrix(y_test,pred))\n",
        "print()\n",
        "print('Accuracy: ', accuracy_score(y_test,pred))\n",
        "print()\n",
        "\n",
        "#Print the predictions\n",
        "#print('Predicted value: ',model.predict(X_test))\n",
        "\n",
        "#Print Actual Label\n",
        "#print('Actual value: ',y_test)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.83      0.83       102\n",
            "         1.0       0.66      0.63      0.65        52\n",
            "\n",
            "    accuracy                           0.77       154\n",
            "   macro avg       0.74      0.73      0.74       154\n",
            "weighted avg       0.76      0.77      0.77       154\n",
            "\n",
            "Confusion Matrix: \n",
            " [[85 17]\n",
            " [19 33]]\n",
            "\n",
            "Accuracy:  0.7662337662337663\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evSPt7f9uouF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d478fe8b-3c2d-43bb-c893-e9ec7039e417"
      },
      "source": [
        "#Evaluate the test data set\n",
        "\n",
        "#The reason why we have the index 1 after the model.evaluate function is because\n",
        "#the function returns the loss as the first element and the accuracy as the \n",
        "#second element. To only output the accuracy, simply access the second element \n",
        "#(which is indexed by 1, since the first element starts its indexing from 0).\n",
        "model.evaluate(X_test, y_test)[1]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "154/154 [==============================] - 0s 70us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7662337677819389"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLZFbM4AurE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}